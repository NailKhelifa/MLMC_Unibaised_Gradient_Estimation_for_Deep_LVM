{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\textbf{\\textcolor{red}{1. Introduction}}$\n",
    "\n",
    "#### 1.1. **Notations**\n",
    "\n",
    "On se donne un ensemble de données $\\boldsymbol{x} = \\{x^{(i)}\\}_{i=1}^n$ composé de $n$ échantillons i.i.d. d'une variable continue ou discrète $x$ à valeurs dans un espace d'observations $\\boldsymbol{\\mathcal{X}}$ (ainsi $\\boldsymbol{x} \\in \\boldsymbol{\\mathcal{X}}$). Nous supposons que les données sont générées par un processus aléatoire impliquant une variable aléatoire continue non observée $\\boldsymbol{z} = \\{z^{(i)}\\}_{i=1}^n$ à valeur dans un espace d'observations $\\boldsymbol{\\mathcal{Z}}$ (ainsi $\\boldsymbol{z} \\in \\boldsymbol{\\mathcal{Z}}$). Le processus se compose de deux étapes :\n",
    "\n",
    "\n",
    "1. La valeur $z^{(i)}$ est générée à partir d'une distribution a priori $p_{\\theta^*}(z)$ ;\n",
    "2. Une valeur $x^{(i)}$ est générée à partir d'une distribution conditionnelle $p_{\\theta^*}(x|z)$.\n",
    "\n",
    "\n",
    "Nous supposons que la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ proviennent de familles paramétriques de distributions $p_{\\theta}(\\boldsymbol{z})$ et $p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z})$ et on note $\\Theta \\subset \\mathbb{R}^d$ l'espace des paramètres. On connait ainsi l'expression de ces deux distributions. En revanche une grande partie de ce processus nous est cachée : les véritables paramètres $\\theta^*$ ainsi que les valeurs des $\\textbf{variables latentes}$ $z^{(i)}$ nous sont inconnus. \n",
    "\n",
    "Connaissant l'expression de la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ pour tout $\\theta \\in \\Theta$, on peut définir la densité jointe $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})$ sur $\\boldsymbol{\\mathcal{X}} \\times \\boldsymbol{\\mathcal{Z}}$ par : \n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z}) p_{\\theta}(\\boldsymbol{z})\n",
    "$$\n",
    "\n",
    "#### 1.2. **Formulation du problème** \n",
    "\n",
    "***Objectif*** : on cherche à déterminer la vraie valeur du paramètre (ici $\\theta^*$) ainsi que les valeurs des variables latentes. \n",
    "\n",
    "Une approche classique pour apprendre $\\boldsymbol{\\theta}$ est de choisir, si celle-ci existe, la valeur de ce paramètre qui maximise la log-vraisemblance marginale de l’ ́echantillon définie par :\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\theta}) = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\log \\int_{\\boldsymbol{\\mathcal{Z}}} p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z}\n",
    "$$\n",
    "\n",
    "***Problème*** : l'intégrale donnée en ci-dessus est intractable (nous ne pouvons donc pas évaluer ou différencier la vraisemblance marginale).\n",
    "\n",
    "#### 1.3. **Nouvelle approche : méthodes variationnelles Bayésiennes**\n",
    "\n",
    "##### 1.3.1. *Modèle de reconnaissance*\n",
    "\n",
    "Dans le but de résoudre les problèmes ci-dessus, introduisons un **modèle de reconnaissance** (paramétrique) {$ q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x}) : \\phi \\in \\Phi$} avec $\\Phi \\subset \\mathbb{R}^d$. Pour tout $\\phi$, $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$ est choisi comme une approximation de la véritable postérieure intractable $p_{\\theta}(z|x)$. L'idée est donc de proposer une valeur de l'a posteriori (faire une hypothèse) et d'introduire une méthode pour apprendre les paramètres du **modèle de reconnaissance** $\\phi$ conjointement avec les paramètres du **modèle génératif** $\\theta$.\n",
    "\n",
    "##### 1.3.2. *Borne inférieure variationnelle*\n",
    "\n",
    "On observe la réecriture suivante de $\\ell (\\boldsymbol{\\theta})$ : \n",
    "\n",
    "\\begin{align*}\n",
    "\\ell (\\boldsymbol{\\theta}) &= \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\\\\n",
    "&= \\log \\int_{\\boldsymbol{\\mathcal{Z}}} p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z}\\\\\n",
    "&= \\log \\int_{\\boldsymbol{\\mathcal{Z}}} \\frac{p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})d\\boldsymbol{z}\\\\\n",
    "&= \\log \\mathbb{E}[\\frac{p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}]\\\\\n",
    "&\\geq \\mathbb{E}_{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}[\\log \\frac{p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}] \\quad \\text{Inégalité de Jensen}\\\\\n",
    "&= \\mathbb{E}_{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}[\\log \\boldsymbol{w}(\\boldsymbol{x})]\\\\\n",
    "&= \\mathcal{L}(\\boldsymbol{\\theta}, \\phi; \\boldsymbol{x})\n",
    "\\end{align*}\n",
    "\n",
    "Ce dernier terme $\\mathcal{L}(\\boldsymbol{\\theta}, \\phi; \\boldsymbol{x})$ est appelé la $\\textbf{\\textcolor{blue}{borne inférieure variationnelle de l'évidence (ELBO dans l'article)}}$ du point de données $\\boldsymbol{x} \\in \\boldsymbol{\\mathcal{X}}$.\n",
    "\n",
    "##### 1.3.3. *Méthodes variationnelles*\n",
    "\n",
    "L’idée est de différencier et optimiser la borne inférieure à la fois par rapport aux paramètres variationnels $\\phi$ et aux paramètres génératifs $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{\\textcolor{green}{VALEURS POUR LES APPLICATIONS NUMÉRIQUES TOUT AU LONG DU NOTEBOOK}}$ \n",
    "\n",
    "- Le **modèle génératif** est donné par $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z}|\\theta, I) \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, I)$, où $\\boldsymbol{x}$ et $\\boldsymbol{z} \\in \\mathbb{R}^{20}$, de sorte que $p_{\\theta}(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x}|\\theta, 2I)$ et $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$. \n",
    "\n",
    "- La **distribution de l'encodeur** (le modèle de reconnaissance) est $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$, où $\\phi = (A, b)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "### Code du modele génératif\n",
    "def p_theta_xz(x, z, theta):\n",
    "    # Définition des paramètres de la distribution normale\n",
    "    mean_z = theta\n",
    "    mean_x = z\n",
    "    covariance_matrix = np.eye(len(x))  # Matrice de covariance identité\n",
    "    \n",
    "    # Calcul des densités de probabilité\n",
    "    normal_distribution_z = multivariate_normal(mean=mean_z, cov=covariance_matrix)\n",
    "    normal_distribution_x_given_z = multivariate_normal(mean=mean_x, cov=covariance_matrix)\n",
    "    \n",
    "    # Calcul de la distribution jointe\n",
    "    joint_probability = normal_distribution_z.pdf(z) * normal_distribution_x_given_z.pdf(x)\n",
    "    \n",
    "    return joint_probability\n",
    "\n",
    "### Code du modèle de reconnaissance (encodeur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\textbf{\\textcolor{red}{2. Autoencodeur Pondéré par l’Importance (IAWE)}}$\n",
    "\n",
    "Voir l'article séminal sur l'IAWE ([ici](https://arxiv.org/pdf/1509.00519.pdf)).\n",
    "\n",
    "L'IWAE utilise cette architecture avec à la fois un **réseau génératif** (paramètre $\\boldsymbol{\\theta}$) et un **réseau de reconnaissance** (paramètre $\\phi$). **La différence est qu'il est entraîné à maximiser une autre borne inférieure** sur $\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$. En particulier, nous utilisons la borne inférieure suivante, correspondant à l'estimation de pondération d'importance à $k$ échantillons de la log-vraisemblance :\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell_{\\text{IAWE}}^{(k)}(\\boldsymbol{\\theta}, \\phi) &= \\mathbb{E}_{\\boldsymbol{z}_1,\\dots,\\boldsymbol{z}_k \\underset{i.i.d}{\\sim} q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}\\left[\\log \\frac{1}{k} \\sum_{i=1}^{k} \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}_i)}{q_{\\phi}(\\boldsymbol{z}_i | \\boldsymbol{x})}\\right ]\n",
    "\\end{align*}\n",
    "\n",
    "Ici, $\\boldsymbol{z}_1, \\dots, \\boldsymbol{z}_k$ sont échantillonnés indépendamment du modèle de reconnaissance. Le terme à l'intérieur de la somme correspond aux poids d'importance non normalisés pour la distribution conjointe, que nous noterons $w_i = \\frac{p(\\boldsymbol{x}, \\boldsymbol{z}_i)}{q_{\\phi}(\\boldsymbol{z}_i | \\boldsymbol{x})}$. \n",
    "\n",
    "L'optimisation de $\\ell_{\\text{IAWE}}^{(k)}(\\boldsymbol{\\theta}, \\phi)$ est effectuée conjointement en $\\boldsymbol{\\theta}$ et $\\phi$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code IAWE à mettre ici "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, à moins que $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$ corresponde exactement à $p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}|\\boldsymbol{x})$, alors en général $\\nabla_{\\boldsymbol{\\theta}}\\ell_{\\text{IAWE}}^{(k)}(\\boldsymbol{\\theta}, \\phi) \\neq \\nabla_{\\boldsymbol{\\theta}}\\ell(\\boldsymbol{\\theta})$, et donc SGD peut optimiser $\\boldsymbol{\\theta}$ loin du vrai maximiseur de $\\mathcal{L}(\\boldsymbol{\\theta})$ à moins que $k$ soit grand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
