{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">  1. Introduction  </span>\n",
    "\n",
    "#### 1.1. **Notations**\n",
    "\n",
    "On se donne un ensemble de données $\\boldsymbol{x} = \\{x^{(i)}\\}_{i=1}^n$ composé de $n$ échantillons i.i.d. d'une variable continue ou discrète $x$ à valeurs dans un espace d'observations $\\boldsymbol{\\mathcal{X}}$ (ainsi $\\boldsymbol{x} \\in \\boldsymbol{\\mathcal{X}}$). Nous supposons que les données sont générées par un processus aléatoire impliquant une variable aléatoire continue non observée $\\boldsymbol{z} = \\{z^{(i)}\\}_{i=1}^n$ à valeur dans un espace d'observations $\\boldsymbol{\\mathcal{Z}}$ (ainsi $\\boldsymbol{z} \\in \\boldsymbol{\\mathcal{Z}}$). Le processus se compose de deux étapes :\n",
    "\n",
    "\n",
    "1. La valeur $z^{(i)}$ est générée à partir d'une distribution a priori $p_{\\theta^*}(z)$ ;\n",
    "2. Une valeur $x^{(i)}$ est générée à partir d'une distribution conditionnelle $p_{\\theta^*}(x|z)$.\n",
    "\n",
    "\n",
    "Nous supposons que la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ proviennent de familles paramétriques de distributions $p_{\\theta}(\\boldsymbol{z})$ et $p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z})$ et on note $\\Theta \\subset \\mathbb{R}^d$ l'espace des paramètres. On connait ainsi l'expression de ces deux distributions. En revanche une grande partie de ce processus nous est cachée : les véritables paramètres $\\theta^*$ ainsi que les valeurs des $\\textbf{variables latentes}$ $z^{(i)}$ nous sont inconnus. \n",
    "\n",
    "Connaissant l'expression de la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ pour tout $\\theta \\in \\Theta$, on peut définir la densité jointe $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})$ sur $\\boldsymbol{\\mathcal{X}} \\times \\boldsymbol{\\mathcal{Z}}$ par : \n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z}) p_{\\theta}(\\boldsymbol{z})\n",
    "$$\n",
    "\n",
    "#### 1.2. **Formulation du problème** \n",
    "\n",
    "***Objectif*** : on cherche à déterminer la vraie valeur du paramètre (ici $\\theta^*$) ainsi que les valeurs des variables latentes. \n",
    "\n",
    "Une approche classique pour apprendre $\\boldsymbol{\\theta}$ est de choisir, si celle-ci existe, la valeur de ce paramètre qui maximise la log-vraisemblance marginale de l’ ́echantillon définie par :\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\theta}) = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\log \\int_{\\boldsymbol{\\mathcal{Z}}} p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z}\n",
    "$$\n",
    "\n",
    "***Problème*** : l'intégrale donnée en ci-dessus est intractable (nous ne pouvons donc pas évaluer ou différencier la vraisemblance marginale).\n",
    "\n",
    "#### 1.3. **Nouvelle approche : méthodes variationnelles Bayésiennes**\n",
    "\n",
    "##### 1.3.1. *Modèle de reconnaissance*\n",
    "\n",
    "Dans le but de résoudre les problèmes ci-dessus, introduisons un **modèle de reconnaissance** (paramétrique) {$ q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x}) : \\phi \\in \\Phi$} avec $\\Phi \\subset \\mathbb{R}^d$. Pour tout $\\phi$, $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$ est choisi comme une approximation de la véritable postérieure intractable $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x})$. L'idée est donc de proposer une valeur de l'a posteriori (faire une hypothèse) et d'introduire une méthode pour apprendre les paramètres du **modèle de reconnaissance** $\\phi$ conjointement avec les paramètres du **modèle génératif** $\\theta$.\n",
    "\n",
    "##### 1.3.2. *Borne inférieure variationnelle*\n",
    "\n",
    "On observe la réecriture suivante de $\\ell (\\boldsymbol{\\theta})$ : \n",
    "\n",
    "\\begin{align*}\n",
    "\\ell (\\boldsymbol{\\theta}) &= \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\\\\n",
    "&= \\log \\int_{\\boldsymbol{\\mathcal{Z}}} p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z}\\\\\n",
    "&= \\log \\int_{\\boldsymbol{\\mathcal{Z}}} \\frac{p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})d\\boldsymbol{z}\\\\\n",
    "&= \\log \\mathbb{E}[\\frac{p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}]\\\\\n",
    "&\\geq \\mathbb{E}_{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}[\\log \\frac{p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}] \\quad \\text{Inégalité de Jensen}\\\\\n",
    "&= \\mathbb{E}_{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}[\\log \\boldsymbol{w}(\\boldsymbol{x})]\\\\\n",
    "&= \\mathcal{L}(\\boldsymbol{\\theta}, \\phi; \\boldsymbol{x})\n",
    "\\end{align*}\n",
    "\n",
    "Ce dernier terme $\\mathcal{L}(\\boldsymbol{\\theta}, \\phi; \\boldsymbol{x})$ est appelé la <span style=\"color:blue\"> borne inférieure variationnelle de l'évidence (ELBO dans l'article) </span>  du point de données $\\boldsymbol{x} \\in \\boldsymbol{\\mathcal{X}}$.\n",
    "\n",
    "##### 1.3.3. *Méthodes variationnelles*\n",
    "\n",
    "L’idée est de différencier et optimiser la borne inférieure à la fois par rapport aux paramètres variationnels $\\phi$ et aux paramètres génératifs $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">  Valeurs pour les applications numériques (tout au long du notebook) </span> \n",
    "\n",
    "(Application numérique inspirée de cet [article](https://arxiv.org/pdf/1802.04537.pdf))\n",
    "\n",
    "- Le **modèle génératif** est donné par $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z}|\\theta, I) \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, I)$, où $\\boldsymbol{x}$ et $\\boldsymbol{z} \\in \\mathbb{R}^{20}$, de sorte que $p_{\\theta}(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x}|\\theta, 2I)$ et $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$. \n",
    "\n",
    "- La **distribution de l'encodeur** (le modèle de reconnaissance) est $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$, où $\\phi = (A, b)$.\n",
    "\n",
    "- Nous considérons des perturbations aléatoires des paramètres près de la valeur optimale par une distribution gaussienne de moyenne nulle et d'écart-type $0.01$.\n",
    "\n",
    "**Dans ce cas, on peut analytiquement calculer la vraie log-vraisemblance du modèle pour quantifier le biais et la variance de tous les estimateurs**.\n",
    "\n",
    "Puisque $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$ est censé être une approximation de la véritable loi a posteriori donnée par $p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} | \\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par tirer le $\\theta$* ($\\in \\mathbb{R}^{20}$) qui sera le paramètre que l'on cherchera à estimer par la suite. \n",
    "\n",
    "A la manière de la génération des données dans l'article Rainforth et al. (2018) : $\\theta^* \\sim \\mathcal{N}(0, I_d)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur de theta à estimer est 1.61\n"
     ]
    }
   ],
   "source": [
    "#theta_true = np.random.multivariate_normal(np.zeros(20), np.identity(20))\n",
    "theta_true = np.random.normal(0, 1)\n",
    "print(f\"La valeur de theta à estimer est {int(theta_true*100)/100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, on va tirer un échantillon $((Z_i, X_i))_i \\stackrel{iid}{\\sim} \\mathcal{N}(\\boldsymbol{z}|\\theta, \\boldsymbol{I}) * \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, \\boldsymbol{I})$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MODÈLE GÉNÉRATIF (LOI JOINTE) ######\n",
    "\n",
    "def joint_probability(theta, dim=20):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    IDÉE : génère un échantillon i.i.d de taille 'size' selon la densité jointe du vecteur aléatoire (x, z).\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    ARGUMENTS :\n",
    "    - size: Taille de l'échantillon\n",
    "    - dim: Dimension des vecteurs x et z ; dim = 20 car on prend x, z dans R^20\n",
    "    - theta: Moyenne de la distribution N(z | theta, Id)\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    OUTPUTS :\n",
    "    - Un échantillon i.i.d de taille 'size' selon la densité jointe\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    # Génère un échantillon Z = (Z_1,...,Z_size) suivant la loi marginale précisée dans l'article\n",
    "    z = np.random.multivariate_normal(np.zeros(dim) + theta, np.identity(dim), size=1)\n",
    "\n",
    "    # Génère l'échantillon X = (X_1,...,X_20) suivant la loi conditionnelle à Z :  N(x | z, Id)\n",
    "    x = np.random.multivariate_normal(z, np.eye(dim), size=1)\n",
    "\n",
    "    return x, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant question de tirer notre encodeur tel que : $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$, où $\\phi = (A, b)$\n",
    "\n",
    "Étant donné un ensemble de données $(X_n)_{n=1}^N$, nous pouvons calculer analytiquement l'optimum de notre objectif $J(\\theta,\\phi)$, donnant $\\theta := \\frac{1}{N} \\sum_{n=1}^N X_n$ et $\\phi^* := (A^*, b^*)$, où $A^* = \\frac{1}{2}\\boldsymbol{I}_{20} \\in \\mathbb{R}^{20 \\times 20}$ et $b^* = [\\frac{\\theta^*}{2}, ..., \\frac{\\theta^*}{2}]^T \\in \\mathbb{R}^{20} $.\n",
    "\n",
    "Nous nous plaçons dans un cas où on aurait réussi à inférer de manière convenable la loi $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$, mais pas parfaitement. Ainsi,  on introduit de la même façon que dans l'article [Tighter Variational Bounds are Not Necessarily Better](https://arxiv.org/pdf/1802.04537.pdf) (Rainforth et al., 2018) une perturbation autour de la valeur optimale du paramètre $\\phi$. Cette perturbation se caractérise par le fait chaque dimension de chaque paramètre a été décalée de manière aléatoire par rapport à sa valeur optimale en utilisant une gaussienne centrée en zéro avec un écart-type de 0,01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noised_params(A, b, std_dev=0.01, dim=20):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    IDÉÉ : Perturber chaque composante de A et de b par une loi normale centrée et d'écart type std_dev.\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    ARGUMENTS :\n",
    "\n",
    "    - A: Matrice dans R^{20x20}\n",
    "    - b: Vecteur dans R^20\n",
    "    - std_dev: Écart type de la loi normale à utiliser pour la perturbation (par défaut 0.01)\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    OUTPUTS :\n",
    "    - noised_A: np.ndarray de shape (20,20) ; Matrice perturbée dans R^{20x20}\n",
    "    - noised_b: np.ndarray de shape (1, 20) ; Vecteur perturbé dans R^20\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perturber chaque composante de A\n",
    "    noised_A = A + np.random.normal(loc=0, scale=std_dev, size=(dim, dim))\n",
    "    \n",
    "    # Perturber chaque composante de b\n",
    "    noised_b = b + np.random.normal(loc=0, scale=std_dev, size=dim)\n",
    "    \n",
    "    return noised_A, noised_b\n",
    "\n",
    "\n",
    "def generate_encoder(x, k, dim=20): ## on oublie l'idée generate_encoder(x_sample, A, b, dim=20) --> on a une expression explicite\n",
    "                                        ## de A et b \n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    IDÉE : Génère un échantillon i.i.d. z_sample selon la loi N(Ax + b, 2/3 * I) dans R^20, à partir d'un échantillon i.i.d. \n",
    "           x_sample.\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    ARGUMENTS :\n",
    "\n",
    "    - x : Échantillon i.i.d. dans R^20\n",
    "    - k : taille de l'échantillon que l'on veut tirer \n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    OUTPUTS :\n",
    "    - Échantillon i.i.d. z_sample selon la loi N(Ax + b, 2/3 * I) dans R^20\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    #A, b = noised_params((1/2)*np.eye(dim), (np.zeros(20) + theta_true)/2) ## on récupère les paramètres perturbés\n",
    "    #Remarque : Dans l'article on ne tire pas avec theta_true mais avec theta_hat\n",
    "\n",
    "    theta_hat = x.mean(axis=0) ## on calcule la moyenne de l'échantillon i.i.d x_sample\n",
    "    \n",
    "    A, b = noised_params((1/2)*np.eye(dim), (np.zeros(20) + theta_hat)/2) ## on récupère les paramètres perturbés\n",
    "    \n",
    "    AX_b = np.dot(x, A.T) + b\n",
    "\n",
    "    cov = (2/3) * np.identity(dim) ## on calcule la variance de la normale multivariée associée à l'encodeur\n",
    "\n",
    "    z_sample = np.random.multivariate_normal(AX_b, cov, size=k)\n",
    "\n",
    "    return z_sample , AX_b #On return AX_b pour pouvoir les utiliser dans la fonction de décodage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque $z_i \\underset{i.i.d}{\\sim} q_{\\phi}(\\cdot|x)$ pour $i = 1, ..., k$, nous obtenons l'estimateur *biaisé* suivant de la log-vraisemblance marginale $\\ell(\\boldsymbol{\\theta}) = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ :\n",
    "\n",
    "$$\n",
    "\\hat{\\ell}^{(k)}(\\boldsymbol{\\theta}) = \\log \\left( \\frac{1}{k} \\sum_{i=1}^{k} w_i \\right),\n",
    "$$\n",
    "\n",
    "où $w_i := w(z_i) = \\frac{p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}_i)}{q_{\\phi}(\\boldsymbol{z}_i|\\boldsymbol{x})}$ est la fonction de poids d'importance.\n",
    "\n",
    "De plus, nous obtenons l'estimateur d'**Importance Sampling Auto-Normalisé (SNIS)** de $\\pi[\\psi]$ pour la fonction de test $\\psi$ :\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}^{(k)}[\\psi] = \\sum_{i=1}^{k} \\frac{w_i}{\\overline{w}} \\psi(z_i),\n",
    "$$\n",
    "\n",
    "où $\\overline{w}_i := \\frac{w(z_i)}{\\sum_{j=1}^{k} w(z_j)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On génère nos échantillons à partir des fonctions précédentes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = joint_probability(theta_true, 1000)\n",
    "X_sample = joint[0] #On prend seulement les X\n",
    "encoder = generate_encoder(X_sample, theta_true)\n",
    "Z_sample = encoder[0] #On prend seulement les Z\n",
    "AX_b = encoder[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(X_sample, Z_sample, AX_b):\n",
    "    q_phi,p_theta = [],[]\n",
    "    theta_hat = X_sample.mean(axis=0) #On prend la moyenne de l'échantillon mais là encore on pourrait s'épargner le tirage de tout l'échantillon\n",
    "    \n",
    "    #Remarque : Avec cette façon de faire on ne fait pas la disctinction entre x_data et Theta_hat -> pas normal\n",
    "    for i in range(len(Z_sample)):\n",
    "        Z_i = Z_sample[i][0]\n",
    "        q_phi.append(multivariate_normal.pdf(Z_i, mean=AX_b[i], cov=(2/3)*np.identity(20)))\n",
    "        p_theta.append(multivariate_normal.pdf(Z_i, mean=theta_hat, cov=np.identity(20)) * multivariate_normal.pdf(X_sample[i], mean=Z_i, cov=np.identity(20)))\n",
    "    \n",
    "    return np.array(p_theta) / np.array(q_phi)\n",
    "\n",
    "def l_hat(x, z, AX_b):\n",
    "    return np.log((1/len(z))*sum(weight(X_sample, Z_sample, AX_b) for i in range(len(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">  2. Autoencodeur Pondéré par l’Importance (IAWE) </span> \n",
    "\n",
    "Voir l'article séminal sur l'IAWE [ici](https://arxiv.org/pdf/1509.00519.pdf).\n",
    "\n",
    "L'IWAE utilise cette architecture avec à la fois un **réseau génératif** (paramètre $\\boldsymbol{\\theta}$) et un **réseau de reconnaissance** (paramètre $\\phi$). **La différence est qu'il est entraîné à maximiser une autre borne inférieure** sur $\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$. En particulier, nous utilisons la borne inférieure suivante, correspondant à l'estimation de pondération d'importance à $k$ échantillons de la log-vraisemblance :\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell_{\\text{IAWE}}^{(k)}(\\boldsymbol{\\theta}, \\phi) &= \\mathbb{E}_{\\boldsymbol{z}_1,\\dots,\\boldsymbol{z}_k \\underset{i.i.d}{\\sim} q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}\\left[\\log \\frac{1}{k} \\sum_{i=1}^{k} \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}_i)}{q_{\\phi}(\\boldsymbol{z}_i | \\boldsymbol{x})}\\right ]\n",
    "\\end{align*}\n",
    "\n",
    "Ici, $\\boldsymbol{z}_1, \\dots, \\boldsymbol{z}_k$ sont échantillonnés indépendamment du modèle de reconnaissance. Le terme à l'intérieur de la somme correspond aux poids d'importance non normalisés pour la distribution conjointe, que nous noterons $w_i = \\frac{p(\\boldsymbol{x}, \\boldsymbol{z}_i)}{q_{\\phi}(\\boldsymbol{z}_i | \\boldsymbol{x})}$. \n",
    "\n",
    "L'optimisation de $\\ell_{\\text{IAWE}}^{(k)}(\\boldsymbol{\\theta}, \\phi)$ est effectuée conjointement en $\\boldsymbol{\\theta}$ et $\\phi$. \n",
    "\n",
    "Il existe deux propriétés de l'IWAE qui nous permettront de le modifier pour produire un estimateur non biaisé :\n",
    "1. Il est consistant, i.e. lorsque le nombre d'échantillons $k$ augmente, l'espérance de $\\ell_{\\text{IAWE}}^{(k)}(\\boldsymbol{\\theta}, \\phi)$ converge vers $\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ : \n",
    "$$\n",
    "\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\lim_{k \\rightarrow +\\infty} \\mathbb{E}[\\ell_{\\text{IAWE}}^{(k)}(\\boldsymbol{\\theta}, \\phi)]\n",
    "$$\n",
    "\n",
    "2. Il est également croissant en espérance : \n",
    "$$\n",
    "\\mathbb{E}[\\ell_{\\text{IAWE}}^{(k+1)}(\\boldsymbol{\\theta}, \\phi)] \\geq \\mathbb{E}[\\ell_{\\text{IAWE}}^{(k)}(\\boldsymbol{\\theta}, \\phi)]\n",
    "$$ \n",
    "\n",
    "Ces propriétés sont suffisantes pour créer un estimateur non biaisé en utilisant l'estimateur de la roulette russe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code IAWE à mettre ici "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, à moins que $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$ corresponde exactement à $p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}|\\boldsymbol{x})$, alors en général $\\nabla_{\\boldsymbol{\\theta}}\\ell_{\\text{IAWE}}^{(k)}(\\boldsymbol{\\theta}, \\phi) \\neq \\nabla_{\\boldsymbol{\\theta}}\\ell(\\boldsymbol{\\theta})$, et donc SGD peut optimiser $\\boldsymbol{\\theta}$ loin du vrai maximiseur de $\\mathcal{L}(\\boldsymbol{\\theta})$ à moins que $k$ soit grand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">  3. Estimateur de la roulette russe (rr) </span> \n",
    "\n",
    "\n",
    "Pour créer un estimateur non biaisé de la fonction de $\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$, nous utilisons l'estimateur de la roulette russe (voir [Kahn, 1955](https://www.rand.org/content/dam/rand/pubs/papers/2008/P766.pdf)). Cet estimateur est utilisé pour estimer la somme de séries infinies, où l'évaluation de chaque terme de la série nécessite presque sûrement seulement une quantité finie de calcul. Intuitivement, l'estimateur de la roulette russe repose sur une troncature aléatoire et une pondération de chaque terme pour tenir compte de la possibilité de ne pas calculer ces termes.\n",
    "\n",
    "Pour illustrer l'idée, soit $\\tilde{\\Delta}_k$ $k$-ième terme d'une série infinie. Supposons que la somme partielle de la série $\\sum_{k=1}^{+\\infty}\\tilde{\\Delta}_k$ converge vers une quantité que nous souhaitons obtenir. Nous pouvons construire un estimateur simple en calculant toujours le premier terme puis en lançant une pièce $b \\sim$ Bernoulli($q$) pour déterminer si nous arrêtons ou continuons à évaluer les termes restants. Avec une probabilité de $1 − q$, nous calculons le reste de la série. En pondérant à nouveau les termes futurs restants par $\\frac{1}{1-q}$, nous obtenons un estimateur non biaisé :\n",
    "\n",
    "$$\n",
    "\\tilde{Y} = \\tilde{\\Delta}_0 + \\left(\\frac{\\sum_{k=1}^{+\\infty}\\tilde{\\Delta}_k}{1-q}\\right)\\mathbb{1}_{b=0} + (0)\\mathbb{1}_{b=1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\tilde{Y}] = \\tilde{\\Delta}_0 + \\frac{\\sum_{k=1}^{+\\infty}\\tilde{\\Delta}_k}{1-q}(1-q) = \\sum_{k=0}^{+\\infty}\\tilde{\\Delta}_k\n",
    "$$\n",
    "Pour obtenir l'estimateur de la \"roulette russe\" (rr) (Forsythe & Leibler, 1950), nous appliquons cette astuce de manière répétée aux termes restants. En effet, en voyant le nombre de termes comme une variable aléatoire $\\mathcal{K}$ à valeurs dans $1, 2, ...$ à utiliser dans la sommation (c'est-à-dire, le nombre de lancers de pièce réussis) dont la distribution a pour fonction de masse de probabilité $p(\\mathcal{K}) = P(\\mathcal{K} = k)$ de support les entiers positifs. Avec $\\mathcal{K}$ tiré de $p(\\cdot)$, l'estimateur prend la forme :\n",
    "\n",
    "$$\n",
    "\\hat{Y}(\\mathcal{K}) = \\sum_{k=0}^K \\frac{\\tilde{\\Delta}_k}{\\mathbb{P}(\\mathcal{K} \\geq k)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{K} \\sim p(\\cdot)}[\\hat{Y}(\\mathcal{K})] = \\sum_{k=0}^{+\\infty}\\tilde{\\Delta}_k\n",
    "\\tag{*}\n",
    "$$\n",
    "\n",
    "**Cependant, la variance de cet estimateur dépend du choix de p(K) et peut potentiellement être très grande voire infinie.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> 4. Stochastically Unbiased Marginalization Objective (SUMO) </span> \n",
    "\n",
    "\n",
    "Voir l'article séminal [ici](https://arxiv.org/pdf/2004.00353.pdf).\n",
    "\n",
    "L'objectif reste ici toujours d'estimer la log-vraisemblance, ce qui revient, dans le cadre introduit ci-dessus à poser\n",
    "\n",
    "\n",
    "$$\n",
    "I_{\\infty} = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\n",
    "$$\n",
    "\n",
    "Nous pouvons transformer toute série absolument convergente en une série télescopique et appliquer la randomisation de la roulette russe pour former un estimateur stochastique non biaisé. Nous nous concentrons ici sur la borne IWAE décrite précédemment. \n",
    "\n",
    "On pose \n",
    "\\begin{align*}\n",
    "\\Delta^{\\text{SUMO}}_k &:= \\hat{\\ell}^{(k+2)}(\\boldsymbol{\\theta}) - \\hat{\\ell}^{(k+1)}(\\boldsymbol{\\theta})\\\\\n",
    "&:= \\log \\left( \\frac{1}{k+2} \\sum_{i=1}^{k+2} w(\\boldsymbol{z}_i) \\right) - \\log \\left( \\frac{1}{k+1} \\sum_{i=1}^{k+1} w(\\boldsymbol{z}_i) \\right),\n",
    "\\end{align*}\n",
    "\n",
    "où on rappelle que $w(\\boldsymbol{z}) := \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}$.\n",
    "\n",
    "On applique l'équation (*) pour construire notre estimateur, que nous appelons $\\textbf{SUMO}$ (Stochastically Unbiased Marginalization Objective) qui correspond précisément à l'estimateur de la roulette russe pour $\\Delta^{\\text{SUMO}}_k$. Ainsi, \n",
    "$$\n",
    "\\hat{\\ell}^{\\text{SUMO}}(\\boldsymbol{\\theta}) := I_0 + \\sum^K_{k=0} \\frac{\\Delta^{\\text{SUMO}}_k}{P(\\mathcal{K} \\geq k)}\n",
    "$$\n",
    "    \n",
    "où $\\mathcal{K} \\sim p(\\cdot)$ une distrubtion de support dans $\\mathbb{N}$\n",
    "\\end{définition}\n",
    "\n",
    "La troncature aléatoire de la série à l'aide de l'estimateur de la roulette russe signifie que c'est un estimateur non biaisé de la log-vraisemblance marginale, quelle que soit la distribution $p(\\cdot)$, i.e. que :\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{K} \\sim p(\\cdot)}[\\hat{\\ell}^{\\text{SUMO}}(\\boldsymbol{\\theta})] = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\ell (\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code SUMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> 5. Estimateur de vraisemblance non biaisé Multi-Level Monte Carlo (MLMC) </span> \n",
    "\n",
    "La méthodologie du Monte Carlo multiniveau de (voir cet [article](https://web.stanford.edu/~glynn/papers/2015/BlanchetG15.html)) repose sur un schéma alternatif astucieux pour construire $\\Delta_k$, qui peut garantir la construction d'estimateurs non biaisés de $\\ell(\\boldsymbol{\\theta})$ qui admettent une variance finie et peuvent être calculés en un temps attendu fini.\n",
    "\n",
    "Nous désignons par $\\boldsymbol{z}^O_i$, $\\boldsymbol{z}^E_i$ deux séquences indépendantes d'échantillons i.i.d. de $q_\\phi$, où $O$, $E$ désignent respectivement impair (odd), pair (even), et $w^O_i$, $w^E_i$ les poids d'importance correspondants. Ensuite, nous définissons $I_0 = \\hat{\\ell}^{(1)}(\\boldsymbol{\\theta})$ et\n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta^\\text{ML}_k = \\hat{\\ell}^{(2^{k+1})}_{O\\cup E}(\\boldsymbol{\\theta}) - \\frac{1}{2} \\left( \\hat{\\ell}^{(2^k)}_O (\\boldsymbol{\\theta}) + \\hat{\\ell}^{(2^k)}_E (\\boldsymbol{\\theta}) \\right),\n",
    "\\end{align*}\n",
    "\n",
    "où $\\hat{\\ell}^{(2^k)}_O (\\boldsymbol{\\theta}) = \\log \\left(\\frac{1}{2^k} \\sum_{i=1}^{2^k} w(z^{O}_i) \\right)$ est calculé en utilisant les échantillons impairs $\\{z^O_i\\}_{i=1}^{2^k}$, $\\hat{\\ell}^{(2^k)}_E (\\boldsymbol{\\theta}) = \\log \\left(\\frac{1}{2^k} \\sum_{i=1}^{2^k} w(z^{E}_i) \\right)$ en utilisant les échantillons pairs $\\{z^E_i\\}_{i=1}^{2^k}$, et $\\hat{\\ell}^{(2^{k+1})}_{O\\cup E} (\\boldsymbol{\\theta})$ en utilisant $\\{z^O_i\\}_{i=1}^{2^k} \\cup \\{z^E_i\\}_{i=1}^{2^k}$. Nous désignons les estimateurs SS/RR multiniveaux correspondants de $\\hat{\\ell}(\\boldsymbol{\\theta})$ comme $\\hat{\\ell}^\\text{ML-SS}(\\boldsymbol{\\theta})$ et $\\hat{\\ell}^\\text{ML-RR}(\\boldsymbol{\\theta})$, et collectivement comme $\\hat{\\ell}^\\text{ML}(\\boldsymbol{\\theta})$.\n",
    "\n",
    "Ainsi, \n",
    "\n",
    "$$\n",
    "\\hat{\\ell}^\\text{ML-SS}(\\boldsymbol{\\theta}) = I_0 + \\frac{\\Delta^{\\text{ML}}_\\mathcal{K}}{p(\\mathcal{K})}, \\quad \\hat{\\ell}^\\text{ML-RR}(\\boldsymbol{\\theta}) = I_0 + \\sum_{k=0}^{K} \\frac{\\Delta^{\\text{ML}}_k}{\\mathbb{P}(\\mathcal{K} \\geq k)},\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient selon l'axe x:\n",
      "[[ 2.   4.   1. ]\n",
      " [ 0.5 -0.5  1.5]\n",
      " [-1.  -5.   2. ]]\n",
      "\n",
      "Gradient selon l'axe y:\n",
      "[[ 1.   1.5  2. ]\n",
      " [ 3.   1.  -1. ]\n",
      " [-1.   2.5  6. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Créer un tableau 2D\n",
    "arr = np.array([[1, 2, 4],\n",
    "                [3, 6, 5],\n",
    "                [2, 1, 7]])\n",
    "\n",
    "# Calculer le gradient du tableau\n",
    "gradient_x, gradient_y = np.gradient(arr)\n",
    "\n",
    "print(\"Gradient selon l'axe x:\")\n",
    "print(gradient_x)\n",
    "\n",
    "print(\"\\nGradient selon l'axe y:\")\n",
    "print(gradient_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
