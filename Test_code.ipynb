{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal\n",
    "import Estimateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par tirer le $\\theta$* ($\\in \\mathbb{R}$) qui sera le paramètre que l'on cherchera à estimer par la suite. \n",
    "\n",
    "A la manière de la génération des données dans l'article Rainforth et al. (2018) : $\\theta^* \\sim \\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur de theta à estimer est -0.79\n"
     ]
    }
   ],
   "source": [
    "#theta_true = np.random.multivariate_normal(np.zeros(20), np.identity(20))\n",
    "theta_true = np.random.normal(0, 1)\n",
    "print(f\"La valeur de theta à estimer est {int(theta_true*100)/100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, on va tirer un échantillon $((Z_i, X_i))_i \\stackrel{iid}{\\sim} \\mathcal{N}(\\boldsymbol{z}|\\theta, \\boldsymbol{I}) * \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, \\boldsymbol{I})$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'échantillon x observé est :\n",
      " \n",
      " x = [ 0.75194605 -0.71688363 -1.92496183 -0.68940399 -1.48462452 -1.04921858\n",
      " -0.21169595 -0.94145241 -1.04245676 -0.46359699 -2.13424346  0.26302805\n",
      " -0.17534825 -0.26272728 -1.06219956 -1.46255712 -0.63788927 -2.29999826\n",
      " -1.51387711 -2.0365551 ]\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "## On se donne notre échantillon x\n",
    "\n",
    "x, _ = Estimateurs.joint_probability(theta_true)\n",
    "\n",
    "print(f\"L'échantillon x observé est :\\n \\n x = {x}\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci nous suffit pour tester notre classe Estimateurs dont le code se trouve dans le fichier Estimateurs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = Estimateurs.noised_params((1/2)*np.eye(20), (np.zeros(20) + theta_true)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.dot(A, x) + b\n",
    "\n",
    "x = multivariate_normal.rvs(y, (2/3)*np.eye(20))\n",
    "\n",
    "z = Estimateurs.generate_encoder(x, 1, A, b)[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimateur = Estimateurs.Estimateurs(x, theta_true, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean must be 1 dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m estimateur\u001b[39m.\u001b[39;49mgrad_ML_RR(theta_star\u001b[39m=\u001b[39;49mtheta_true, n_simulations\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:368\u001b[0m, in \u001b[0;36mEstimateurs.grad_ML_RR\u001b[0;34m(self, theta_star, n_simulations)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mfor\u001b[39;00m theta_val \u001b[39min\u001b[39;00m theta_values:\n\u001b[1;32m    367\u001b[0m     estimateur \u001b[39m=\u001b[39m Estimateurs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, theta_val, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr)\n\u001b[0;32m--> 368\u001b[0m     ML_RR_values\u001b[39m.\u001b[39mappend(estimateur\u001b[39m.\u001b[39;49mlog_likelihood_ML_RR(n_simulations)[\u001b[39m1\u001b[39m])\n\u001b[1;32m    370\u001b[0m gradient_ML_RR \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgradient(ML_RR_values, theta_values)\n\u001b[1;32m    372\u001b[0m \u001b[39mreturn\u001b[39;00m gradient_ML_RR\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:293\u001b[0m, in \u001b[0;36mEstimateurs.log_likelihood_ML_RR\u001b[0;34m(self, n_simulations)\u001b[0m\n\u001b[1;32m    289\u001b[0m Delta_hat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m])\n\u001b[1;32m    290\u001b[0m Delta_theta \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 293\u001b[0m I_0_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml_hat(z)[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m z \u001b[39min\u001b[39;49;00m z_sample_hat])\n\u001b[1;32m    294\u001b[0m I_0_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_theta])\n\u001b[1;32m    296\u001b[0m \u001b[39m## On clacule l'estimateur de la roulette russe associé à ce delta, c'est celui qui correspond à l'estimateur RR \u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m## et on stocke le résultat dans la liste RR sur laquelle on moyennera en sortie \u001b[39;00m\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:293\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    289\u001b[0m Delta_hat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m])\n\u001b[1;32m    290\u001b[0m Delta_theta \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 293\u001b[0m I_0_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml_hat(z)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_hat])\n\u001b[1;32m    294\u001b[0m I_0_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_theta])\n\u001b[1;32m    296\u001b[0m \u001b[39m## On clacule l'estimateur de la roulette russe associé à ce delta, c'est celui qui correspond à l'estimateur RR \u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m## et on stocke le résultat dans la liste RR sur laquelle on moyennera en sortie \u001b[39;00m\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:196\u001b[0m, in \u001b[0;36mEstimateurs.l_hat\u001b[0;34m(self, z_sample)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mIDEA: compute the biaised estimate of the log-likelihood l_theta(x) that we will later use to build our estimators. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m## calcul de l_theta_hat (pour theta_hat)\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m l_theta_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight(z_sample[i])[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(z_sample)\u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m))))\n\u001b[1;32m    198\u001b[0m \u001b[39m## calcul de l_theta (pour theta)\u001b[39;00m\n\u001b[1;32m    199\u001b[0m l_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight(z_sample[i])[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:196\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mIDEA: compute the biaised estimate of the log-likelihood l_theta(x) that we will later use to build our estimators. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m## calcul de l_theta_hat (pour theta_hat)\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m l_theta_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight(z_sample[i])[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n\u001b[1;32m    198\u001b[0m \u001b[39m## calcul de l_theta (pour theta)\u001b[39;00m\n\u001b[1;32m    199\u001b[0m l_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight(z_sample[i])[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:162\u001b[0m, in \u001b[0;36mEstimateurs.weight\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    159\u001b[0m AX_b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA\u001b[39m.\u001b[39mT) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[1;32m    161\u001b[0m \u001b[39m## calcul des poids pour theta_hat (associé à l'observation x)\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m weight_theta_hat \u001b[39m=\u001b[39m compute_ratio(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, z, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtheta_hat, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mA_hat, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb_hat)\n\u001b[1;32m    164\u001b[0m \u001b[39m## cacul des poids pour theta (associé à un theta)                                                                                                    \u001b[39;00m\n\u001b[1;32m    165\u001b[0m weight_theta \u001b[39m=\u001b[39m compute_ratio(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, z, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:88\u001b[0m, in \u001b[0;36mcompute_ratio\u001b[0;34m(x, z, theta, A, b)\u001b[0m\n\u001b[1;32m     85\u001b[0m I \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meye(\u001b[39m20\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39m# Distribution parameters\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m p_theta_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mmultivariate_normal(theta, \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49mI)\n\u001b[1;32m     89\u001b[0m p_theta_z_given_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mmultivariate_normal((theta \u001b[39m+\u001b[39m x)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39mI)\n\u001b[1;32m     90\u001b[0m q_phi_z_given_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mmultivariate_normal(np\u001b[39m.\u001b[39mdot(A, x) \u001b[39m+\u001b[39m b\u001b[39m.\u001b[39mflatten(), (\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m*\u001b[39mI)\n",
      "File \u001b[0;32mmtrand.pyx:4205\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mean must be 1 dimensional"
     ]
    }
   ],
   "source": [
    "estimateur.grad_ML_RR(theta_star=theta_true, n_simulations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
