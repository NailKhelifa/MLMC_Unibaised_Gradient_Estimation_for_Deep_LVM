{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal\n",
    "import Estimateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par tirer le $\\theta$* ($\\in \\mathbb{R}$) qui sera le paramètre que l'on cherchera à estimer par la suite. \n",
    "\n",
    "A la manière de la génération des données dans l'article Rainforth et al. (2018) : $\\theta^* \\sim \\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur de theta à estimer est 1.74\n"
     ]
    }
   ],
   "source": [
    "#theta_true = np.random.multivariate_normal(np.zeros(20), np.identity(20))\n",
    "theta_true = np.random.normal(0, 1)\n",
    "print(f\"La valeur de theta à estimer est {int(theta_true*100)/100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, on va tirer un échantillon $((Z_i, X_i))_i \\stackrel{iid}{\\sim} \\mathcal{N}(\\boldsymbol{z}|\\theta, \\boldsymbol{I}) * \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, \\boldsymbol{I})$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'échantillon x observé est :\n",
      " \n",
      " x = [ 2.38087258  6.24049352  0.16469592  2.62065357  4.87777977  3.69963942\n",
      "  1.89467462  1.229945    2.23982581  1.43748682  4.88963885  1.53558343\n",
      "  1.19018097 -0.64590077  2.24476187  0.44904091  2.58357178  3.13263801\n",
      " -0.08436885 -2.14311332]\n"
     ]
    }
   ],
   "source": [
    "## On se donne notre échantillon x\n",
    "\n",
    "x, _ = Estimateurs.joint_probability(theta_true)\n",
    "\n",
    "print(f\"L'échantillon x observé est :\\n \\n x = {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci nous suffit pour tester notre classe Estimateurs dont le code se trouve dans le fichier Estimateurs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimateur = Estimateurs.Estimateurs(x, theta_true, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Estimateurs.weight() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m estimateur\u001b[39m.\u001b[39;49mgrad_ML_RR(theta_star\u001b[39m=\u001b[39;49mtheta_true, n_simulations\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:348\u001b[0m, in \u001b[0;36mEstimateurs.grad_ML_RR\u001b[0;34m(self, theta_star, n_simulations)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mfor\u001b[39;00m theta_val \u001b[39min\u001b[39;00m theta_values:\n\u001b[1;32m    347\u001b[0m     estimateur \u001b[39m=\u001b[39m Estimateurs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, theta_val, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr)\n\u001b[0;32m--> 348\u001b[0m     ML_RR_values\u001b[39m.\u001b[39mappend(estimateur\u001b[39m.\u001b[39;49mlog_likelihood_ML_RR(n_simulations)[\u001b[39m1\u001b[39m])\n\u001b[1;32m    350\u001b[0m gradient_ML_RR \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgradient(ML_RR_values, theta_values)\n\u001b[1;32m    352\u001b[0m \u001b[39mreturn\u001b[39;00m gradient_ML_RR\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:273\u001b[0m, in \u001b[0;36mEstimateurs.log_likelihood_ML_RR\u001b[0;34m(self, n_simulations)\u001b[0m\n\u001b[1;32m    269\u001b[0m Delta_hat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m])\n\u001b[1;32m    270\u001b[0m Delta_theta \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 273\u001b[0m I_0_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml_hat(z)[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m z \u001b[39min\u001b[39;49;00m z_sample_hat])\n\u001b[1;32m    274\u001b[0m I_0_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_theta])\n\u001b[1;32m    276\u001b[0m \u001b[39m## On clacule l'estimateur de la roulette russe associé à ce delta, c'est celui qui correspond à l'estimateur RR \u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m## et on stocke le résultat dans la liste RR sur laquelle on moyennera en sortie \u001b[39;00m\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:273\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    269\u001b[0m Delta_hat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m])\n\u001b[1;32m    270\u001b[0m Delta_theta \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 273\u001b[0m I_0_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml_hat(z)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_hat])\n\u001b[1;32m    274\u001b[0m I_0_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_theta])\n\u001b[1;32m    276\u001b[0m \u001b[39m## On clacule l'estimateur de la roulette russe associé à ce delta, c'est celui qui correspond à l'estimateur RR \u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m## et on stocke le résultat dans la liste RR sur laquelle on moyennera en sortie \u001b[39;00m\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:176\u001b[0m, in \u001b[0;36mEstimateurs.l_hat\u001b[0;34m(self, z_sample)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mIDEA: compute the biaised estimate of the log-likelihood l_theta(x) that we will later use to build our estimators. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m## calcul de l_theta_hat (pour theta_hat)\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m l_theta_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, z_sample[i])[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(z_sample)\u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m))))\n\u001b[1;32m    178\u001b[0m \u001b[39m## calcul de l_theta (pour theta)\u001b[39;00m\n\u001b[1;32m    179\u001b[0m l_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, z_sample[i])[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:176\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mIDEA: compute the biaised estimate of the log-likelihood l_theta(x) that we will later use to build our estimators. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m## calcul de l_theta_hat (pour theta_hat)\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m l_theta_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, z_sample[i])[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n\u001b[1;32m    178\u001b[0m \u001b[39m## calcul de l_theta (pour theta)\u001b[39;00m\n\u001b[1;32m    179\u001b[0m l_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, z_sample[i])[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n",
      "\u001b[0;31mTypeError\u001b[0m: Estimateurs.weight() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "estimateur.grad_ML_RR(theta_star=theta_true, n_simulations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
