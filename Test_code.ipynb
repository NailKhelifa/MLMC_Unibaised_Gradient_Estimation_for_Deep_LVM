{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal\n",
    "import Estimateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par tirer le $\\theta$* ($\\in \\mathbb{R}$) qui sera le paramètre que l'on cherchera à estimer par la suite. \n",
    "\n",
    "A la manière de la génération des données dans l'article Rainforth et al. (2018) : $\\theta^* \\sim \\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur de theta à estimer est -0.78\n"
     ]
    }
   ],
   "source": [
    "#theta_true = np.random.multivariate_normal(np.zeros(20), np.identity(20))\n",
    "theta_true = np.random.normal(0, 1)\n",
    "print(f\"La valeur de theta à estimer est {int(theta_true*100)/100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, on va tirer un échantillon $((Z_i, X_i))_i \\stackrel{iid}{\\sim} \\mathcal{N}(\\boldsymbol{z}|\\theta, \\boldsymbol{I}) * \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, \\boldsymbol{I})$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'échantillon x observé est :\n",
      " \n",
      " x = [ 1.70774145  2.82837553 -2.88026079 -1.34320377  1.0914359  -1.07333702\n",
      " -0.46133858  0.2926711   1.15821776 -2.99224668  1.39791198  0.57449632\n",
      "  1.17813917 -1.99321111  1.66744699  0.582609   -2.21034668 -1.38173675\n",
      "  0.9646666  -2.27223744]\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "## On se donne notre échantillon x\n",
    "\n",
    "x, _ = Estimateurs.joint_probability(theta_true)\n",
    "\n",
    "print(f\"L'échantillon x observé est :\\n \\n x = {x}\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci nous suffit pour tester notre classe Estimateurs dont le code se trouve dans le fichier Estimateurs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimateur = Estimateurs.Estimateurs(x, theta_true, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m estimateur\u001b[39m.\u001b[39;49mgrad_ML_RR(theta_star\u001b[39m=\u001b[39;49mtheta_true, n_simulations\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:364\u001b[0m, in \u001b[0;36mEstimateurs.grad_ML_RR\u001b[0;34m(self, theta_star, n_simulations)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mfor\u001b[39;00m theta_val \u001b[39min\u001b[39;00m theta_values:\n\u001b[1;32m    363\u001b[0m     estimateur \u001b[39m=\u001b[39m Estimateurs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, theta_val, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr)\n\u001b[0;32m--> 364\u001b[0m     ML_RR_values\u001b[39m.\u001b[39mappend(estimateur\u001b[39m.\u001b[39;49mlog_likelihood_ML_RR(n_simulations)[\u001b[39m1\u001b[39m])\n\u001b[1;32m    366\u001b[0m gradient_ML_RR \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgradient(ML_RR_values, theta_values)\n\u001b[1;32m    368\u001b[0m \u001b[39mreturn\u001b[39;00m gradient_ML_RR\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:289\u001b[0m, in \u001b[0;36mEstimateurs.log_likelihood_ML_RR\u001b[0;34m(self, n_simulations)\u001b[0m\n\u001b[1;32m    285\u001b[0m Delta_hat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m])\n\u001b[1;32m    286\u001b[0m Delta_theta \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 289\u001b[0m I_0_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml_hat(z)[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m z \u001b[39min\u001b[39;49;00m z_sample_hat])\n\u001b[1;32m    290\u001b[0m I_0_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_theta])\n\u001b[1;32m    292\u001b[0m \u001b[39m## On clacule l'estimateur de la roulette russe associé à ce delta, c'est celui qui correspond à l'estimateur RR \u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m## et on stocke le résultat dans la liste RR sur laquelle on moyennera en sortie \u001b[39;00m\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:289\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    285\u001b[0m Delta_hat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m])\n\u001b[1;32m    286\u001b[0m Delta_theta \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 289\u001b[0m I_0_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml_hat(z)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_hat])\n\u001b[1;32m    290\u001b[0m I_0_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_theta])\n\u001b[1;32m    292\u001b[0m \u001b[39m## On clacule l'estimateur de la roulette russe associé à ce delta, c'est celui qui correspond à l'estimateur RR \u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m## et on stocke le résultat dans la liste RR sur laquelle on moyennera en sortie \u001b[39;00m\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:192\u001b[0m, in \u001b[0;36mEstimateurs.l_hat\u001b[0;34m(self, z_sample)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mIDEA: compute the biaised estimate of the log-likelihood l_theta(x) that we will later use to build our estimators. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m## calcul de l_theta_hat (pour theta_hat)\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m l_theta_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight(z_sample[i])[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(z_sample)\u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m))))\n\u001b[1;32m    194\u001b[0m \u001b[39m## calcul de l_theta (pour theta)\u001b[39;00m\n\u001b[1;32m    195\u001b[0m l_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight(z_sample[i])[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:192\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mIDEA: compute the biaised estimate of the log-likelihood l_theta(x) that we will later use to build our estimators. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m## calcul de l_theta_hat (pour theta_hat)\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m l_theta_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight(z_sample[i])[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n\u001b[1;32m    194\u001b[0m \u001b[39m## calcul de l_theta (pour theta)\u001b[39;00m\n\u001b[1;32m    195\u001b[0m l_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight(z_sample[i])[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:158\u001b[0m, in \u001b[0;36mEstimateurs.weight\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    155\u001b[0m AX_b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA\u001b[39m.\u001b[39mT) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[1;32m    157\u001b[0m \u001b[39m## calcul des poids pour theta_hat (associé à l'observation x)\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m weight_theta_hat \u001b[39m=\u001b[39m poids(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, z, AX_b_hat)\n\u001b[1;32m    160\u001b[0m \u001b[39m## cacul des poids pour theta (associé à un theta)                                                                                                    \u001b[39;00m\n\u001b[1;32m    161\u001b[0m weight_theta \u001b[39m=\u001b[39m poids(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, z, AX_b)\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:89\u001b[0m, in \u001b[0;36mpoids\u001b[0;34m(x, z, AX_b)\u001b[0m\n\u001b[1;32m     86\u001b[0m theta_hat \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m#On prend la moyenne de l'échantillon mais là encore on pourrait s'épargner le tirage de tout l'échantillon\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m#Remarque : Avec cette façon de faire on ne fait pas la disctinction entre x_data et Theta_hat -> pas normal\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39;49m(z)):\n\u001b[1;32m     90\u001b[0m     Z_i \u001b[39m=\u001b[39m z[i][\u001b[39m0\u001b[39m]\n\u001b[1;32m     91\u001b[0m     q_phi\u001b[39m.\u001b[39mappend(multivariate_normal\u001b[39m.\u001b[39mpdf(Z_i, mean\u001b[39m=\u001b[39mAX_b[i], cov\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39midentity(\u001b[39m20\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "estimateur.grad_ML_RR(theta_star=theta_true, n_simulations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
