{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">  **SIMULATIONS ET MÉTHODES DE MONTE CARLO**  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal\n",
    "import Estimateurs\n",
    "import Code\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **NOTATIONS**  </span> : on se donne un ensemble de données $\\boldsymbol{x} = \\{x^{(i)}\\}_{i=1}^n$ composé de $n$ échantillons i.i.d. d'une variable continue ou discrète $x$ à valeurs dans un espace d'observations $\\boldsymbol{\\mathcal{X}}$ (ainsi $\\boldsymbol{x} \\in \\boldsymbol{\\mathcal{X}}$). Nous supposons que les données sont générées par un processus aléatoire impliquant une variable aléatoire continue non observée $\\boldsymbol{z} = \\{z^{(i)}\\}_{i=1}^n$ à valeur dans un espace d'observations $\\boldsymbol{\\mathcal{Z}}$ (ainsi $\\boldsymbol{z} \\in \\boldsymbol{\\mathcal{Z}}$). Le processus se compose de deux étapes :\n",
    "\n",
    "\n",
    "1. La valeur $z^{(i)}$ est générée à partir d'une distribution a priori $p_{\\theta^*}(z)$ ;\n",
    "2. Une valeur $x^{(i)}$ est générée à partir d'une distribution conditionnelle $p_{\\theta^*}(x|z)$.\n",
    "\n",
    "\n",
    "Nous supposons que la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ proviennent de familles paramétriques de distributions $p_{\\theta}(\\boldsymbol{z})$ et $p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z})$ et on note $\\Theta \\subset \\mathbb{R}^d$ l'espace des paramètres. On connait ainsi l'expression de ces deux distributions. En revanche une grande partie de ce processus nous est cachée : les véritables paramètres $\\theta^*$ ainsi que les valeurs des $\\textbf{variables latentes}$ $z^{(i)}$ nous sont inconnus. \n",
    "\n",
    "Connaissant l'expression de la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ pour tout $\\theta \\in \\Theta$, on peut définir la densité jointe $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})$ sur $\\boldsymbol{\\mathcal{X}} \\times \\boldsymbol{\\mathcal{Z}}$ par : \n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z}) p_{\\theta}(\\boldsymbol{z})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **OBJECTIF**  </span> : on cherche à déterminer la vraie valeur du paramètre (ici $\\theta^*$) ainsi que les valeurs des variables latentes. \n",
    "\n",
    "Une approche classique pour apprendre $\\boldsymbol{\\theta}$ est de choisir, si celle-ci existe, la valeur de ce paramètre qui maximise la log-vraisemblance marginale de l’ ́echantillon définie par :\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\theta}) = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\log \\int_{\\boldsymbol{\\mathcal{Z}}} p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **PROBLÈME**  </span> : l'intégrale donnée en ci-dessus est intractable (nous ne pouvons donc pas évaluer ou différencier la vraisemblance marginale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **SOLUTION**  </span> : dans le but de résoudre les problèmes ci-dessus, introduisons un **modèle de reconnaissance** (paramétrique) {$ q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x}) : \\phi \\in \\Phi$} avec $\\Phi \\subset \\mathbb{R}^d$. Pour tout $\\phi$, $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$ est choisi comme une approximation de la véritable postérieure intractable $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x})$. L'idée est donc de proposer une valeur de l'a posteriori (faire une hypothèse) et d'introduire une méthode pour apprendre les paramètres du **modèle de reconnaissance** $\\phi$ conjointement avec les paramètres du **modèle génératif** $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">  **Partie 0. Définition du cadre gaussien pour la génération des données**  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Application numérique inspirée de [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf))\n",
    "\n",
    "- Le **modèle génératif** est donné par $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z}|\\theta, I) \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, I)$, où $\\boldsymbol{x}$ et $\\boldsymbol{z} \\in \\mathbb{R}^{20}$, de sorte que $p_{\\theta}(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x}|\\theta, 2I)$ et $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$. \n",
    "\n",
    "- La **distribution de l'encodeur** (le modèle de reconnaissance) est $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$, où $\\phi = (A, b)$.\n",
    "\n",
    "- Nous considérons des perturbations aléatoires des paramètres près de la valeur optimale par une distribution gaussienne de moyenne nulle et d'écart-type $0.01$.\n",
    "\n",
    "**Dans ce cas, on peut analytiquement calculer la vraie log-vraisemblance du modèle pour quantifier le biais et la variance de tous les estimateurs**.\n",
    "\n",
    "Puisque $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$ est censé être une approximation de la véritable loi a posteriori donnée par $p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} | \\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par tirer le $\\theta$* ($\\in \\mathbb{R}$) qui sera le paramètre que l'on cherchera à estimer par la suite. \n",
    "\n",
    "A la manière de la génération des données dans [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf) : $\\theta^* \\sim \\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur de theta à estimer est -0.88\n"
     ]
    }
   ],
   "source": [
    "#theta_true = np.random.multivariate_normal(np.zeros(20), np.identity(20))\n",
    "theta_true = np.random.normal(0, 1)\n",
    "print(f\"La valeur de theta à estimer est {int(theta_true*100)/100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée est désormais de tirer une observation $\\boldsymbol{x} \\in \\mathbb{R}^{20}$, chose que l'on peut faire puisque toutes les distributions nous sont données. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'échantillon x observé est :\n",
      " \n",
      " x = [ 1.21615218 -0.27236525 -3.98166696 -0.69853942 -0.45024338 -1.11288876\n",
      " -2.76122173 -1.21131542 -1.58362649 -1.52781937  1.43659677  1.23667594\n",
      " -1.53491302 -0.58626718  0.37933809 -1.25275817  1.26155415 -0.15981144\n",
      " -0.78238294 -1.00001909] \n",
      " \n",
      " et on vérifique bien sa taille est celle voulue : (20,)\n"
     ]
    }
   ],
   "source": [
    "## On se donne notre échantillon x\n",
    "\n",
    "x, _ = Estimateurs.joint_probability(theta_true)\n",
    "\n",
    "print(f\"L'échantillon x observé est :\\n \\n x = {x} \\n \\n et on vérifique bien sa taille est celle voulue : {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant question de tirer notre encodeur tel que : $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$, où $\\phi = (A, b)$. **Le problème qui se pose est donc celui du choix de $A$ et de $b$**. Dans l'article [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf), il nous est indiqué : \n",
    "\n",
    "*\"Following [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf), we consider random perturbations of the parameters near the optimal value by a zero-mean Gaussian with standard deviation 0.01\"*\n",
    "\n",
    "Ainsi, étant donné une observation $\\boldsymbol{x}$,  le choix parfait pour $\\phi$ serait $\\phi^* = (A^*, b^*)$ où $A^* = \\frac{1}{2}\\boldsymbol{I}_{20} \\in \\mathbb{R}^{20 \\times 20}$ et $b^* = [\\frac{\\theta^*}{2}, ..., \\frac{\\theta^*}{2}]^T \\in \\mathbb{R}^{20} $. Suivant l'indication dans l'article, nous nous plaçons dans un cas où on aurait réussi à inférer de manière convenable la loi $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$, mais pas parfaitement. Ainsi,  on introduit de la même façon que dans l'article [[Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf) une perturbation autour de la valeur optimale du paramètre $\\phi$. Cette perturbation se caractérise par le fait chaque dimension de chaque paramètre a été décalée de manière aléatoire par rapport à sa valeur optimale en utilisant une gaussienne centrée en zéro avec un écart-type de 0,01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On vérifie que les tailles de A, b et x coincident :\n",
      " \n",
      " taille A : (20, 20) \n",
      " taille b : (20,) \n",
      " taille x : (20,)\n"
     ]
    }
   ],
   "source": [
    "dim = 20 \n",
    "\n",
    "## On calcule les valeurs optimales de A et de b\n",
    "A = 0.5 * np.eye(dim)\n",
    "b = 0.5 * theta_true * np.ones(dim)\n",
    "\n",
    "## On calcule les valeurs perturbées de A et de b, qui sont FIXÉES dans la suite\n",
    "noised_A, noised_b = Code.noised_params(A, b)\n",
    "\n",
    "print(f\"On vérifie que les tailles de A, b et x coincident :\\n \\n taille A : {A.shape} \\n taille b : {b.shape} \\n taille x : {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:23<00:00, 86.23it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-34.66322971802261"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 0.6\n",
    "n_simulations = 2000\n",
    "\n",
    "estimator = Code.log_likelihood_SUMO(r, theta_true, x, noised_A, noised_b, n_simulations)\n",
    "\n",
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.45755889033261"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Code.true_likelihood(x, theta_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.random.geometric(0.6)\n",
    "\n",
    "def generate_encoder(x, k, noised_A, noised_b): ## on oublie l'idée generate_encoder(x_sample, A, b, dim=20) --> on a une expression explicite\n",
    "                                        ## de A et b \n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    IDÉE : Génère un échantillon i.i.d. z_sample de taille k selon la loi N(noised_Ax + noised_b, 2/3 * I) dans R^20, à \n",
    "           partir d'un échantillon i.i.d. x_sample.\n",
    "    ---------------------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    #A, b = noised_params((1/2)*np.eye(dim), (np.zeros(20) + theta_true)/2) ## on récupère les paramètres perturbés\n",
    "    #Remarque : Dans l'article on ne tire pas avec theta_true mais avec theta_hat\n",
    "        \n",
    "    dim = 20\n",
    "\n",
    "    AX_b = np.array(np.dot(noised_A, x) + noised_b).T\n",
    "\n",
    "    cov = (2/3) * np.identity(dim) ## on calcule la variance de la normale multivariée associée à l'encodeur\n",
    "\n",
    "    z_sample = []\n",
    "\n",
    "    for _ in range(2**(k)):\n",
    "        z_sample.append(np.random.multivariate_normal(AX_b, cov)) ## 2**(k+1) pour ne pas avoir de problèmes avec les échantillons pairs \n",
    "                                                                  ## et impairs\n",
    "        \n",
    "    z_odd = z_sample[1::2]\n",
    "    z_even = z_sample[::2]\n",
    "\n",
    "    ## POUR\n",
    "    return z_sample , z_odd, z_even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, _, _ = generate_encoder(x, K, noised_A, noised_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.34994842, -0.82143932, -1.36983564, -1.65138517, -1.81744937,\n",
       "        -0.0049065 , -2.05363711, -1.03501354, -1.33831304,  0.52427565,\n",
       "        -1.17511552, -0.74076219, -1.99531659, -1.38701059,  0.99056051,\n",
       "        -1.38576043, -1.73857967, -1.87410153, -3.47398806, -1.34074499]),\n",
       " array([ 0.72800562, -0.39466995, -1.08102283, -1.59713597, -0.97037914,\n",
       "        -0.79627537, -1.16577633, -1.09399926, -2.79201295, -1.52980946,\n",
       "        -1.54255341, -1.32792181, -1.04594005, -1.91995223, -1.51967009,\n",
       "        -1.96507907, -1.25559216, -0.49416158, -3.05020563, -1.86662301]),\n",
       " array([-0.60694879, -1.01197756, -1.99349378, -2.31235375, -1.62767947,\n",
       "        -2.27048219, -1.74572724, -1.4284252 , -0.27368761, -1.49041326,\n",
       "        -0.31088099, -0.3663779 , -2.31212403, -0.59258822, -1.47557925,\n",
       "        -2.05253382, -2.8314969 , -1.01952956, -2.28105075, -0.69507099]),\n",
       " array([ 0.58367025, -0.89916432, -0.68463897, -1.86180276, -2.30744232,\n",
       "        -1.88002002, -2.60665478, -2.52045563,  0.37578685, -1.47053402,\n",
       "        -1.22669997, -1.68617206, -1.29679157, -1.52095669, -2.46406562,\n",
       "        -1.27121111, -0.14044469, -0.43711635, -1.13474457, -1.35063124])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
