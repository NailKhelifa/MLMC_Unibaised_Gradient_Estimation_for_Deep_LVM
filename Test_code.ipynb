{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">  **SIMULATIONS ET MÉTHODES DE MONTE CARLO**  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal\n",
    "import Estimateurs\n",
    "import Code\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **NOTATIONS**  </span> : on se donne un ensemble de données $\\boldsymbol{x} = \\{x^{(i)}\\}_{i=1}^n$ composé de $n$ échantillons i.i.d. d'une variable continue ou discrète $x$ à valeurs dans un espace d'observations $\\boldsymbol{\\mathcal{X}}$ (ainsi $\\boldsymbol{x} \\in \\boldsymbol{\\mathcal{X}}$). Nous supposons que les données sont générées par un processus aléatoire impliquant une variable aléatoire continue non observée $\\boldsymbol{z} = \\{z^{(i)}\\}_{i=1}^n$ à valeur dans un espace d'observations $\\boldsymbol{\\mathcal{Z}}$ (ainsi $\\boldsymbol{z} \\in \\boldsymbol{\\mathcal{Z}}$). Le processus se compose de deux étapes :\n",
    "\n",
    "\n",
    "1. La valeur $z^{(i)}$ est générée à partir d'une distribution a priori $p_{\\theta^*}(z)$ ;\n",
    "2. Une valeur $x^{(i)}$ est générée à partir d'une distribution conditionnelle $p_{\\theta^*}(x|z)$.\n",
    "\n",
    "\n",
    "Nous supposons que la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ proviennent de familles paramétriques de distributions $p_{\\theta}(\\boldsymbol{z})$ et $p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z})$ et on note $\\Theta \\subset \\mathbb{R}^d$ l'espace des paramètres. On connait ainsi l'expression de ces deux distributions. En revanche une grande partie de ce processus nous est cachée : les véritables paramètres $\\theta^*$ ainsi que les valeurs des $\\textbf{variables latentes}$ $z^{(i)}$ nous sont inconnus. \n",
    "\n",
    "Connaissant l'expression de la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ pour tout $\\theta \\in \\Theta$, on peut définir la densité jointe $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})$ sur $\\boldsymbol{\\mathcal{X}} \\times \\boldsymbol{\\mathcal{Z}}$ par : \n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z}) p_{\\theta}(\\boldsymbol{z})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **OBJECTIF**  </span> : on cherche à déterminer la vraie valeur du paramètre (ici $\\theta^*$) ainsi que les valeurs des variables latentes. \n",
    "\n",
    "Une approche classique pour apprendre $\\boldsymbol{\\theta}$ est de choisir, si celle-ci existe, la valeur de ce paramètre qui maximise la log-vraisemblance marginale de l’ ́echantillon définie par :\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\theta}) = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\log \\int_{\\boldsymbol{\\mathcal{Z}}} p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **PROBLÈME**  </span> : l'intégrale donnée en ci-dessus est intractable (nous ne pouvons donc pas évaluer ou différencier la vraisemblance marginale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **SOLUTION**  </span> : dans le but de résoudre les problèmes ci-dessus, introduisons un **modèle de reconnaissance** (paramétrique) {$ q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x}) : \\phi \\in \\Phi$} avec $\\Phi \\subset \\mathbb{R}^d$. Pour tout $\\phi$, $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$ est choisi comme une approximation de la véritable postérieure intractable $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x})$. L'idée est donc de proposer une valeur de l'a posteriori (faire une hypothèse) et d'introduire une méthode pour apprendre les paramètres du **modèle de reconnaissance** $\\phi$ conjointement avec les paramètres du **modèle génératif** $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">  **Partie 0. Définition du cadre gaussien pour la génération des données**  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Application numérique inspirée de [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf))\n",
    "\n",
    "- Le **modèle génératif** est donné par $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z}|\\theta, I) \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, I)$, où $\\boldsymbol{x}$ et $\\boldsymbol{z} \\in \\mathbb{R}^{20}$, de sorte que $p_{\\theta}(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x}|\\theta, 2I)$ et $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$. \n",
    "\n",
    "- La **distribution de l'encodeur** (le modèle de reconnaissance) est $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$, où $\\phi = (A, b)$.\n",
    "\n",
    "- Nous considérons des perturbations aléatoires des paramètres près de la valeur optimale par une distribution gaussienne de moyenne nulle et d'écart-type $0.01$.\n",
    "\n",
    "**Dans ce cas, on peut analytiquement calculer la vraie log-vraisemblance du modèle pour quantifier le biais et la variance de tous les estimateurs**.\n",
    "\n",
    "Puisque $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$ est censé être une approximation de la véritable loi a posteriori donnée par $p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} | \\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par tirer le $\\theta$* ($\\in \\mathbb{R}$) qui sera le paramètre que l'on cherchera à estimer par la suite. \n",
    "\n",
    "A la manière de la génération des données dans [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf) : $\\theta^* \\sim \\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur de theta à estimer est 0.1\n"
     ]
    }
   ],
   "source": [
    "#theta_true = np.random.multivariate_normal(np.zeros(20), np.identity(20))\n",
    "theta_true = np.random.normal(0, 1)\n",
    "print(f\"La valeur de theta à estimer est {int(theta_true*100)/100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée est désormais de tirer une observation $\\boldsymbol{x} \\in \\mathbb{R}^{20}$, chose que l'on peut faire puisque toutes les distributions nous sont données. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'échantillon x observé est :\n",
      " \n",
      " x = [ 1.60750558 -2.16030643 -0.3107593   0.0213147  -0.88663846  0.38341634\n",
      " -1.65068801  0.45419634 -3.13300075  0.98046782  2.20926844  0.83651449\n",
      " -0.20560276  2.0740473  -0.37142195  1.48174024  1.76824852  0.9408015\n",
      "  1.79676813 -0.75270061] \n",
      " \n",
      " et on vérifique bien sa taille est celle voulue : (20,)\n"
     ]
    }
   ],
   "source": [
    "## On se donne notre échantillon x\n",
    "\n",
    "x, _ = Estimateurs.joint_probability(theta_true)\n",
    "\n",
    "print(f\"L'échantillon x observé est :\\n \\n x = {x} \\n \\n et on vérifique bien sa taille est celle voulue : {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant question de tirer notre encodeur tel que : $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$, où $\\phi = (A, b)$. **Le problème qui se pose est donc celui du choix de $A$ et de $b$**. Dans l'article [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf), il nous est indiqué : \n",
    "\n",
    "*\"Following [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf), we consider random perturbations of the parameters near the optimal value by a zero-mean Gaussian with standard deviation 0.01\"*\n",
    "\n",
    "Ainsi, étant donné une observation $\\boldsymbol{x}$,  le choix parfait pour $\\phi$ serait $\\phi^* = (A^*, b^*)$ où $A^* = \\frac{1}{2}\\boldsymbol{I}_{20} \\in \\mathbb{R}^{20 \\times 20}$ et $b^* = [\\frac{\\theta^*}{2}, ..., \\frac{\\theta^*}{2}]^T \\in \\mathbb{R}^{20} $. Suivant l'indication dans l'article, nous nous plaçons dans un cas où on aurait réussi à inférer de manière convenable la loi $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$, mais pas parfaitement. Ainsi,  on introduit de la même façon que dans l'article [[Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf) une perturbation autour de la valeur optimale du paramètre $\\phi$. Cette perturbation se caractérise par le fait chaque dimension de chaque paramètre a été décalée de manière aléatoire par rapport à sa valeur optimale en utilisant une gaussienne centrée en zéro avec un écart-type de 0,01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On vérifie que les tailles de A, b et x coincident :\n",
      " \n",
      " taille A : (20, 20) \n",
      " taille b : (20,) \n",
      " taille x : (20,)\n"
     ]
    }
   ],
   "source": [
    "dim = 20 \n",
    "\n",
    "## On calcule les valeurs optimales de A et de b\n",
    "A = 0.5 * np.eye(dim)\n",
    "b = 0.5 * theta_true * np.ones(dim)\n",
    "\n",
    "## On calcule les valeurs perturbées de A et de b, qui sont FIXÉES dans la suite\n",
    "noised_A, noised_b = Code.noised_params(A, b)\n",
    "\n",
    "print(f\"On vérifie que les tailles de A, b et x coincident :\\n \\n taille A : {A.shape} \\n taille b : {b.shape} \\n taille x : {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 284.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-36.31462874400653"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 0.6\n",
    "n_simulations = 10\n",
    "\n",
    "estimator = Code.log_likelihood_SUMO(r, theta_true, x, noised_A, noised_b, n_simulations)\n",
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
