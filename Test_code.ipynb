{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">  **SIMULATIONS ET MÉTHODES DE MONTE CARLO**  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal\n",
    "import Estimateurs\n",
    "import Code\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **NOTATIONS**  </span> : on se donne un ensemble de données $\\boldsymbol{x} = \\{x^{(i)}\\}_{i=1}^n$ composé de $n$ échantillons i.i.d. d'une variable continue ou discrète $x$ à valeurs dans un espace d'observations $\\boldsymbol{\\mathcal{X}}$ (ainsi $\\boldsymbol{x} \\in \\boldsymbol{\\mathcal{X}}$). Nous supposons que les données sont générées par un processus aléatoire impliquant une variable aléatoire continue non observée $\\boldsymbol{z} = \\{z^{(i)}\\}_{i=1}^n$ à valeur dans un espace d'observations $\\boldsymbol{\\mathcal{Z}}$ (ainsi $\\boldsymbol{z} \\in \\boldsymbol{\\mathcal{Z}}$). Le processus se compose de deux étapes :\n",
    "\n",
    "\n",
    "1. La valeur $z^{(i)}$ est générée à partir d'une distribution a priori $p_{\\theta^*}(z)$ ;\n",
    "2. Une valeur $x^{(i)}$ est générée à partir d'une distribution conditionnelle $p_{\\theta^*}(x|z)$.\n",
    "\n",
    "\n",
    "Nous supposons que la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ proviennent de familles paramétriques de distributions $p_{\\theta}(\\boldsymbol{z})$ et $p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z})$ et on note $\\Theta \\subset \\mathbb{R}^d$ l'espace des paramètres. On connait ainsi l'expression de ces deux distributions. En revanche une grande partie de ce processus nous est cachée : les véritables paramètres $\\theta^*$ ainsi que les valeurs des $\\textbf{variables latentes}$ $z^{(i)}$ nous sont inconnus. \n",
    "\n",
    "Connaissant l'expression de la distribution a priori $p_{\\theta^*}(\\boldsymbol{z})$ et la vraisemblance $p_{\\theta^*}(\\boldsymbol{x}|\\boldsymbol{z})$ pour tout $\\theta \\in \\Theta$, on peut définir la densité jointe $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})$ sur $\\boldsymbol{\\mathcal{X}} \\times \\boldsymbol{\\mathcal{Z}}$ par : \n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = p_{\\theta}(\\boldsymbol{x}|\\boldsymbol{z}) p_{\\theta}(\\boldsymbol{z})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **OBJECTIF**  </span> : on cherche à déterminer la vraie valeur du paramètre (ici $\\theta^*$) ainsi que les valeurs des variables latentes. \n",
    "\n",
    "Une approche classique pour apprendre $\\boldsymbol{\\theta}$ est de choisir, si celle-ci existe, la valeur de ce paramètre qui maximise la log-vraisemblance marginale de l’ ́echantillon définie par :\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\theta}) = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\log \\int_{\\boldsymbol{\\mathcal{Z}}} p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **PROBLÈME**  </span> : l'intégrale donnée en ci-dessus est intractable (nous ne pouvons donc pas évaluer ou différencier la vraisemblance marginale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">  **SOLUTION**  </span> : dans le but de résoudre les problèmes ci-dessus, introduisons un **modèle de reconnaissance** (paramétrique) {$ q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x}) : \\phi \\in \\Phi$} avec $\\Phi \\subset \\mathbb{R}^d$. Pour tout $\\phi$, $q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})$ est choisi comme une approximation de la véritable postérieure intractable $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x})$. L'idée est donc de proposer une valeur de l'a posteriori (faire une hypothèse) et d'introduire une méthode pour apprendre les paramètres du **modèle de reconnaissance** $\\phi$ conjointement avec les paramètres du **modèle génératif** $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">  **Partie 0. Définition du cadre gaussien pour la génération des données**  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Application numérique inspirée de [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf))\n",
    "\n",
    "- Le **modèle génératif** est donné par $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z}|\\theta, I) \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, I)$, où $\\boldsymbol{x}$ et $\\boldsymbol{z} \\in \\mathbb{R}^{20}$, de sorte que $p_{\\theta}(\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{x}|\\theta, 2I)$ et $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$. \n",
    "\n",
    "- La **distribution de l'encodeur** (le modèle de reconnaissance) est $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$, où $\\phi = (A, b)$.\n",
    "\n",
    "- Nous considérons des perturbations aléatoires des paramètres près de la valeur optimale par une distribution gaussienne de moyenne nulle et d'écart-type $0.01$.\n",
    "\n",
    "**Dans ce cas, on peut analytiquement calculer la vraie log-vraisemblance du modèle pour quantifier le biais et la variance de tous les estimateurs**.\n",
    "\n",
    "Puisque $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$ est censé être une approximation de la véritable loi a posteriori donnée par $p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} | \\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"> 0.1. **Génération du vrai $\\theta$**  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par tirer le $\\theta$* ($\\in \\mathbb{R}$) qui sera le paramètre que l'on cherchera à estimer par la suite. \n",
    "\n",
    "A la manière de la génération des données dans [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf) : $\\theta^* \\sim \\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur de theta à estimer est -0.54\n"
     ]
    }
   ],
   "source": [
    "#theta_true = np.random.multivariate_normal(np.zeros(20), np.identity(20))\n",
    "theta_true = np.random.normal(0, 1)\n",
    "print(f\"La valeur de theta à estimer est {int(theta_true*100)/100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"> 0.2. **Génération de notre vecteur d'observation $\\boldsymbol{x} \\in \\mathbb{R}^{20}$**  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée est désormais de tirer une observation $\\boldsymbol{x} \\in \\mathbb{R}^{20}$, chose que l'on peut faire puisque toutes les distributions nous sont données. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'échantillon x observé est :\n",
      " \n",
      " x = [ 0.97338888 -2.73170457 -1.6806929   1.05083576 -2.21507264 -3.18502551\n",
      " -2.23967998 -0.45062161 -1.76321727  3.15293986  0.40634669 -0.67576346\n",
      " -0.70036704  2.22620484 -3.09627883 -0.47438378 -2.50192095 -1.12050425\n",
      " -0.64540774 -2.3607701 ] \n",
      " \n",
      " et on vérifique bien sa taille est celle voulue : (20,)\n"
     ]
    }
   ],
   "source": [
    "## On se donne notre échantillon x\n",
    "\n",
    "x, _ = Estimateurs.joint_probability(theta_true)\n",
    "\n",
    "print(f\"L'échantillon x observé est :\\n \\n x = {x} \\n \\n et on vérifique bien sa taille est celle voulue : {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"> 0.3. **Génération de la vraie log-vraisemblance et du vrai gradient associé à $\\theta_{\\text{true}}$ et à notre observation $\\boldsymbol{x}$**  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur associée à la log-vraisemblance de l'échantillon x pour theta_true est : -34.070631965652126 \n",
      " \n",
      "La valeur associée au gradient de l'échantillon x pour theta_true est : \n",
      "\n",
      " [-0.75740826  1.09513846  0.56963263 -0.7961317   0.83682249  1.32179893\n",
      "  0.84912617 -0.04540302  0.61089481 -1.84718375 -0.47388717  0.06716791\n",
      "  0.07946969 -1.38381624  1.27742559 -0.03352193  0.98024665  0.2895383\n",
      "  0.05199004  0.90967123] \n",
      " \n"
     ]
    }
   ],
   "source": [
    "true_likelihood = Code.true_likelihood(x, theta_true)\n",
    "true_gradient = Code.true_grad(x, theta_true)\n",
    "\n",
    "print(f\"La valeur associée à la log-vraisemblance de l'échantillon x pour theta_true est : {true_likelihood} \\n \")\n",
    "print(f\"La valeur associée au gradient de l'échantillon x pour theta_true est : \\n\\n {true_gradient} \\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"> 0.4. **Génération des paramètres de l'encodeur $A \\in \\mathbb{R}^{20\\times 20}$ et $b \\in \\mathbb{R}^{20}$**  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant question de tirer notre encodeur tel que : $q_{\\phi}(z|x) = \\mathcal{N}\\left(\\boldsymbol{z}|A\\boldsymbol{x} + b, \\frac{2}{3}I \\right)$, où $\\phi = (A, b)$. **Le problème qui se pose est donc celui du choix de $A$ et de $b$**. Dans l'article [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf), il nous est indiqué : \n",
    "\n",
    "*\"Following [Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf), we consider random perturbations of the parameters near the optimal value by a zero-mean Gaussian with standard deviation 0.01\"*\n",
    "\n",
    "Ainsi, étant donné une observation $\\boldsymbol{x}$,  le choix parfait pour $\\phi$ serait $\\phi^* = (A^*, b^*)$ où $A^* = \\frac{1}{2}\\boldsymbol{I}_{20} \\in \\mathbb{R}^{20 \\times 20}$ et $b^* = [\\frac{\\theta^*}{2}, ..., \\frac{\\theta^*}{2}]^T \\in \\mathbb{R}^{20} $. Suivant l'indication dans l'article, nous nous plaçons dans un cas où on aurait réussi à inférer de manière convenable la loi $p_{\\theta}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}\\left( \\frac{\\theta + x}{2}, \\frac{1}{2}I \\right)$, mais pas parfaitement. Ainsi,  on introduit de la même façon que dans l'article [[Rainforth et al. (2018)](https://arxiv.org/pdf/1802.04537.pdf) une perturbation autour de la valeur optimale du paramètre $\\phi$. Cette perturbation se caractérise par le fait chaque dimension de chaque paramètre a été décalée de manière aléatoire par rapport à sa valeur optimale en utilisant une gaussienne centrée en zéro avec un écart-type de 0,01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On vérifie que les tailles de A, b et x coincident :\n",
      " \n",
      " taille A : (20, 20) \n",
      " taille b : (20,) \n",
      " taille x : (20,)\n"
     ]
    }
   ],
   "source": [
    "dim = 20 \n",
    "\n",
    "## On calcule les valeurs optimales de A et de b\n",
    "A = 0.5 * np.eye(dim)\n",
    "b = 0.5 * theta_true * np.ones(dim)\n",
    "\n",
    "## On calcule les valeurs perturbées de A et de b, qui sont FIXÉES dans la suite\n",
    "noised_A, noised_b = Code.noised_params(A, b)\n",
    "\n",
    "print(f\"On vérifie que les tailles de A, b et x coincident :\\n \\n taille A : {A.shape} \\n taille b : {b.shape} \\n taille x : {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">  **Partie 1. L'estimateur SUMO (Stochastically Unbiased Marginalization Objective (SUMO))**  </span>\n",
    "\n",
    "### <span style=\"color:orange\">  **1.1 Les estimateurs RR (Roulette Russe) et SS (Single Sample)**  </span>\n",
    "\n",
    "On rappelle rapidement la manière dont sont définis les estimateurs RR et SS, utilisés plus tard pour construire SUMO, ML-SS et ML-RR.  \n",
    "Désignons une quantité d'intérêt par $I_\\infty = \\log p_\\theta(\\boldsymbol{x})$. Supposons que $I_\\infty$ puisse être écrite comme\n",
    "\n",
    "$$\n",
    "I_\\infty = \\mathbb{E}[I_0] + \\sum_{k=0}^\\infty \\mathbb{E}[\\Delta_k] \n",
    "$$\n",
    "\n",
    "pour les variables aléatoires $I_0$ et $(\\Delta_k)_{k\\geq0}$. Nous pouvons estimer $I_\\infty$ de manière non biaisée via les estimateurs suivants ss ou rr :\n",
    "\n",
    "$$\n",
    "ss = I_0 + \\frac{\\Delta_K}{p(K)}, \\quad rr = I_0 + \\sum_{k=0}^K \\frac{\\Delta_k}{\\mathbb{P}(K \\geq k)}\n",
    "$$\n",
    "\n",
    "où $K \\sim Geom(r)$.\n",
    "\n",
    "### <span style=\"color:orange\">  **1.2 L'estimateurs SUMO (Stochastically Unbiased Marginalization Objective (SUMO))**  </span>\n",
    "\n",
    "On pose \n",
    "\\begin{align*}\n",
    "\\Delta^{\\text{SUMO}}_k &:= \\hat{\\ell}^{(k+2)}(\\boldsymbol{\\theta}) - \\hat{\\ell}^{(k+1)}(\\boldsymbol{\\theta})\\\\\n",
    "&:= \\log \\left( \\frac{1}{k+2} \\sum_{i=1}^{k+2} w(\\boldsymbol{z}_i) \\right) - \\log \\left( \\frac{1}{k+1} \\sum_{i=1}^{k+1} w(\\boldsymbol{z}_i) \\right),\n",
    "\\end{align*}\n",
    "\n",
    "où on rappelle que $w(\\boldsymbol{z}) := \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z})}{q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}$.\n",
    "\n",
    "On applique l'estimateur $RR$ pour construire notre estimateur, que nous appelons $\\textbf{SUMO}$ (Stochastically Unbiased Marginalization Objective) qui correspond précisément à l'estimateur de la roulette russe pour $\\Delta^{\\text{SUMO}}_k$. Ainsi, \n",
    "$$\n",
    "\\hat{\\ell}^{\\text{SUMO}}(\\boldsymbol{\\theta}) := I_0 + \\sum^K_{k=0} \\frac{\\Delta^{\\text{SUMO}}_k}{P(\\mathcal{K} \\geq k)}\n",
    "$$\n",
    "    \n",
    "où $\\mathcal{K} \\sim p(\\cdot)$ une distrubtion de support dans $\\mathbb{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 25.21it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'generator' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mn_simulations) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m theta \u001b[39min\u001b[39;00m theta_values:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         estimated_likelihood\u001b[39m.\u001b[39mappend(Code\u001b[39m.\u001b[39;49mlog_likelihood_SUMO(r, theta, x, noised_A, noised_b, n_simulations))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m true_likelihood_values \u001b[39m=\u001b[39m [true_likelihood(x, theta) \u001b[39mfor\u001b[39;00m theta \u001b[39min\u001b[39;00m theta_values]\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Code.py:166\u001b[0m, in \u001b[0;36mlog_likelihood_SUMO\u001b[0;34m(r, theta, x, noised_A, noised_b, n_simulations)\u001b[0m\n\u001b[1;32m    162\u001b[0m         SUMO\u001b[39m.\u001b[39mappend(I_0 \u001b[39m+\u001b[39m \u001b[39msum\u001b[39m(Delta_theta(j)\u001b[39m/\u001b[39m((\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mr)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mj)) \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, K))\n\u001b[1;32m    164\u001b[0m         pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)  \u001b[39m# Update the progress bar\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean([np\u001b[39m.\u001b[39;49mmean(values) \u001b[39mfor\u001b[39;49;00m values \u001b[39min\u001b[39;49;00m SUMO])\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Code.py:166\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m         SUMO\u001b[39m.\u001b[39mappend(I_0 \u001b[39m+\u001b[39m \u001b[39msum\u001b[39m(Delta_theta(j)\u001b[39m/\u001b[39m((\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mr)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mj)) \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, K))\n\u001b[1;32m    164\u001b[0m         pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)  \u001b[39m# Update the progress bar\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean([np\u001b[39m.\u001b[39;49mmean(values) \u001b[39mfor\u001b[39;00m values \u001b[39min\u001b[39;00m SUMO])\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3501\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3502\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3504\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3505\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/_methods.py:131\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    129\u001b[0m         ret \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype(ret \u001b[39m/\u001b[39m rcount)\n\u001b[1;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     ret \u001b[39m=\u001b[39m ret \u001b[39m/\u001b[39;49m rcount\n\u001b[1;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'generator' and 'int'"
     ]
    }
   ],
   "source": [
    "n_simulations = 10 \n",
    "r = 0.6\n",
    "\n",
    "theta_min = theta_true - 5  # Limite inférieure de la plage\n",
    "theta_max = theta_true + 5 # Limite supérieure de la plage\n",
    "num_points = 30  # Nombre de points à générer\n",
    "theta_values = np.linspace(theta_min, theta_max, num_points)\n",
    "\n",
    "estimated_likelihood = []\n",
    "\n",
    "with tqdm(total=n_simulations) as pbar:\n",
    "\n",
    "    for theta in theta_values:\n",
    "\n",
    "        estimated_likelihood.append(Code.log_likelihood_SUMO(r, theta, x, noised_A, noised_b, n_simulations))\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "true_likelihood_values = [true_likelihood(x, theta) for theta in theta_values]\n",
    "\n",
    "plt.plot(theta_values, true_likelihood_values, color='r', label='True likelihood')  \n",
    "plt.scatter(theta_values, estimated_likelihood, color='purple', marker='x', label='SUMO')\n",
    "plt.axvline(x=theta_true, color='black', linestyle='--', label='theta='+ str('{:.2f}'.format(theta_true)))\n",
    "plt.ylim([-300,500])\n",
    "plt.xlabel('Theta')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.title('Estimation de la likelihood par SUMO')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
