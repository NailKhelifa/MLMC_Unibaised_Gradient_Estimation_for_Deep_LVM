{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.stats import multivariate_normal\n",
    "import Estimateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par tirer le $\\theta$* ($\\in \\mathbb{R}$) qui sera le paramètre que l'on cherchera à estimer par la suite. \n",
    "\n",
    "A la manière de la génération des données dans l'article Rainforth et al. (2018) : $\\theta^* \\sim \\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur de theta à estimer est 0.58\n"
     ]
    }
   ],
   "source": [
    "#theta_true = np.random.multivariate_normal(np.zeros(20), np.identity(20))\n",
    "theta_true = np.random.normal(0, 1)\n",
    "print(f\"La valeur de theta à estimer est {int(theta_true*100)/100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, on va tirer un échantillon $((Z_i, X_i))_i \\stackrel{iid}{\\sim} \\mathcal{N}(\\boldsymbol{z}|\\theta, \\boldsymbol{I}) * \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{z}, \\boldsymbol{I})$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'échantillon x observé est :\n",
      " \n",
      " x = [ 2.20422981  2.26469783  0.85109594 -0.72115318  0.74210777  2.58821114\n",
      " -0.04561904 -0.65143135 -1.66634061 -0.92748446 -1.14232242  0.46471566\n",
      " -0.61563554  0.08950375  2.2530738   0.53737847  2.12590487  3.57317853\n",
      "  1.56709784 -1.29055781]\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "## On se donne notre échantillon x\n",
    "\n",
    "x, _ = Estimateurs.joint_probability(theta_true)\n",
    "\n",
    "print(f\"L'échantillon x observé est :\\n \\n x = {x}\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci nous suffit pour tester notre classe Estimateurs dont le code se trouve dans le fichier Estimateurs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimateur = Estimateurs.Estimateurs(x, theta_true, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 400 into shape (1,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Test_code.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m estimateur\u001b[39m.\u001b[39;49mgrad_ML_RR(theta_star\u001b[39m=\u001b[39;49mtheta_true, n_simulations\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:348\u001b[0m, in \u001b[0;36mEstimateurs.grad_ML_RR\u001b[0;34m(self, theta_star, n_simulations)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mfor\u001b[39;00m theta_val \u001b[39min\u001b[39;00m theta_values:\n\u001b[1;32m    347\u001b[0m     estimateur \u001b[39m=\u001b[39m Estimateurs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, theta_val, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr)\n\u001b[0;32m--> 348\u001b[0m     ML_RR_values\u001b[39m.\u001b[39mappend(estimateur\u001b[39m.\u001b[39;49mlog_likelihood_ML_RR(n_simulations)[\u001b[39m1\u001b[39m])\n\u001b[1;32m    350\u001b[0m gradient_ML_RR \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgradient(ML_RR_values, theta_values)\n\u001b[1;32m    352\u001b[0m \u001b[39mreturn\u001b[39;00m gradient_ML_RR\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:273\u001b[0m, in \u001b[0;36mEstimateurs.log_likelihood_ML_RR\u001b[0;34m(self, n_simulations)\u001b[0m\n\u001b[1;32m    269\u001b[0m Delta_hat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m])\n\u001b[1;32m    270\u001b[0m Delta_theta \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 273\u001b[0m I_0_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml_hat(z)[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m z \u001b[39min\u001b[39;49;00m z_sample_hat])\n\u001b[1;32m    274\u001b[0m I_0_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_theta])\n\u001b[1;32m    276\u001b[0m \u001b[39m## On clacule l'estimateur de la roulette russe associé à ce delta, c'est celui qui correspond à l'estimateur RR \u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m## et on stocke le résultat dans la liste RR sur laquelle on moyennera en sortie \u001b[39;00m\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:273\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    269\u001b[0m Delta_hat \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_hat[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m0\u001b[39m])\n\u001b[1;32m    270\u001b[0m Delta_theta \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_odd_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z_sample_even_theta[:\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(k)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 273\u001b[0m I_0_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml_hat(z)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_hat])\n\u001b[1;32m    274\u001b[0m I_0_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_hat(z)[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m z \u001b[39min\u001b[39;00m z_sample_theta])\n\u001b[1;32m    276\u001b[0m \u001b[39m## On clacule l'estimateur de la roulette russe associé à ce delta, c'est celui qui correspond à l'estimateur RR \u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m## et on stocke le résultat dans la liste RR sur laquelle on moyennera en sortie \u001b[39;00m\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:176\u001b[0m, in \u001b[0;36mEstimateurs.l_hat\u001b[0;34m(self, z_sample)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mIDEA: compute the biaised estimate of the log-likelihood l_theta(x) that we will later use to build our estimators. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m## calcul de l_theta_hat (pour theta_hat)\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m l_theta_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight(z_sample[i])[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(z_sample)\u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m))))\n\u001b[1;32m    178\u001b[0m \u001b[39m## calcul de l_theta (pour theta)\u001b[39;00m\n\u001b[1;32m    179\u001b[0m l_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight(z_sample[i])[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:176\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mIDEA: compute the biaised estimate of the log-likelihood l_theta(x) that we will later use to build our estimators. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m## calcul de l_theta_hat (pour theta_hat)\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m l_theta_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight(z_sample[i])[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n\u001b[1;32m    178\u001b[0m \u001b[39m## calcul de l_theta (pour theta)\u001b[39;00m\n\u001b[1;32m    179\u001b[0m l_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog((\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mlen\u001b[39m(z_sample)) \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight(z_sample[i])[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(z_sample)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/MLMC_Unibaised_Gradient_Estimation_for_Deep_LVM/Estimateurs.py:142\u001b[0m, in \u001b[0;36mEstimateurs.weight\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m---------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39mIDEA: compute importance weights according to the formula w_i = w(z_i) = p_theta(x, z_i)/q_phi(z_i|x)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m## calcul des poids pour theta_hat (associé à l'observation x)\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m weight_theta_hat \u001b[39m=\u001b[39m multivariate_normal\u001b[39m.\u001b[39;49mpdf(z, mean\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtheta_hat, cov\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49midentity(\u001b[39m20\u001b[39;49m)) \u001b[39m*\u001b[39m multivariate_normal\u001b[39m.\u001b[39mpdf(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, mean\u001b[39m=\u001b[39mz, cov\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39midentity(\u001b[39m20\u001b[39m)) \u001b[39m/\u001b[39m  multivariate_normal\u001b[39m.\u001b[39mpdf(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA_hat) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_hat, cov\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39midentity(\u001b[39m20\u001b[39m))\n\u001b[1;32m    144\u001b[0m \u001b[39m## cacul des poids pour theta (associé à un theta)                                                                                                    \u001b[39;00m\n\u001b[1;32m    145\u001b[0m weight_theta \u001b[39m=\u001b[39m multivariate_normal\u001b[39m.\u001b[39mpdf(z, mean\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta, cov\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39midentity(\u001b[39m20\u001b[39m)) \u001b[39m*\u001b[39m multivariate_normal\u001b[39m.\u001b[39mpdf(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, mean\u001b[39m=\u001b[39mz, cov\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39midentity(\u001b[39m20\u001b[39m)) \u001b[39m/\u001b[39m  multivariate_normal\u001b[39m.\u001b[39mpdf(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb, cov\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39midentity(\u001b[39m20\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/scipy/stats/_multivariate.py:585\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.pdf\u001b[0;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpdf\u001b[39m(\u001b[39mself\u001b[39m, x, mean\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cov\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, allow_singular\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    567\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Multivariate normal probability density function.\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \n\u001b[1;32m    569\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m \n\u001b[1;32m    584\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_parameters(mean, cov, allow_singular)\n\u001b[1;32m    586\u001b[0m     dim, mean, cov_object \u001b[39m=\u001b[39m params\n\u001b[1;32m    587\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_quantiles(x, dim)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/scipy/stats/_multivariate.py:415\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._process_parameters\u001b[0;34m(self, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_parameters_Covariance(mean, cov)\n\u001b[1;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[39m# Before `Covariance` classes were introduced,\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[39m# `multivariate_normal` accepted plain arrays as `cov` and used the\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[39m# following input validation. To avoid disturbing the behavior of\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[39m# `multivariate_normal` when plain arrays are used, we use the\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[39m# original input validation here.\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m     dim, mean, cov \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_parameters_psd(\u001b[39mNone\u001b[39;49;00m, mean, cov)\n\u001b[1;32m    416\u001b[0m     \u001b[39m# After input validation, some methods then processed the arrays\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[39m# with a `_PSD` object and used that to perform computation.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[39m# To avoid branching statements in each method depending on whether\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# `cov` is an array or `Covariance` object, we always process the\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39m# array with `_PSD`, and then use wrapper that satisfies the\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[39m# `Covariance` interface, `CovViaPSD`.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     psd \u001b[39m=\u001b[39m _PSD(cov, allow_singular\u001b[39m=\u001b[39mallow_singular)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/scipy/stats/_multivariate.py:469\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._process_parameters_psd\u001b[0;34m(self, dim, mean, cov)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    468\u001b[0m     mean \u001b[39m=\u001b[39m mean\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 469\u001b[0m     cov \u001b[39m=\u001b[39m cov\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    471\u001b[0m \u001b[39mif\u001b[39;00m mean\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m mean\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m dim:\n\u001b[1;32m    472\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mArray \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a vector of length \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    473\u001b[0m                      dim)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 400 into shape (1,1)"
     ]
    }
   ],
   "source": [
    "estimateur.grad_ML_RR(theta_star=theta_true, n_simulations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
